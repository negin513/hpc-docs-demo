{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"location":"","text":"User Documentation for NCAR High Performance Computing \u00b6 This is the home of user documentation for the NCAR high-performance computing (HPC) and storage resources that CISL manages. The knowledge base includes searchable information specific to HPC resources, storage systems, authentication procedures and others, as well as additional how-to articles and troubleshooting articles. \u00b6 Selected Links \u00b6 Derecho Casper Cheyenne Don't find what you need? Log in here to submit a help request: NCAR Research Computing You need a CIT password to submit a request. Call 303-497-2400 if you don't have one. Other CISL web pages \u00b6 CISL Home page Help Portal JupyterHub Tip The NCAR HPC Users Group (NHUG) is a reouce group for all users of NCAR HPC resources. All users are welcome to join the NHUG Slack workspace . CISL welcomes your contributions This project is hosted on GitHub and your contributions are welcome! This page is just a copy of https://arc.ucar.edu/knowledge_base_documentation in mkdown for demonstrations purposes.","title":"Home"},{"location":"#user-documentation-for-ncar-high-performance-computing","text":"This is the home of user documentation for the NCAR high-performance computing (HPC) and storage resources that CISL manages. The knowledge base includes searchable information specific to HPC resources, storage systems, authentication procedures and others, as well as additional how-to articles and troubleshooting articles.","title":"User Documentation for NCAR High Performance Computing"},{"location":"#_1","text":"","title":""},{"location":"#selected-links","text":"Derecho Casper Cheyenne Don't find what you need? Log in here to submit a help request: NCAR Research Computing You need a CIT password to submit a request. Call 303-497-2400 if you don't have one.","title":"Selected Links"},{"location":"#other-cisl-web-pages","text":"CISL Home page Help Portal JupyterHub Tip The NCAR HPC Users Group (NHUG) is a reouce group for all users of NCAR HPC resources. All users are welcome to join the NHUG Slack workspace . CISL welcomes your contributions This project is hosted on GitHub and your contributions are welcome! This page is just a copy of https://arc.ucar.edu/knowledge_base_documentation in mkdown for demonstrations purposes.","title":"Other CISL web pages"},{"location":"contributing/","text":"Contribution Guide \u00b6 Welcome to the NCAR HPC Resources GitHub repository! This guide provides an overview of how to contribute to this project and the standards to follow when adding content to this repository. Our goal is to create a comprehensive and user-friendly documentation resource for NCAR's HPC resources. Repository Overview \u00b6 This repository contains technical documentation for NCAR HPC resources. The documentation is written in Markdown, which is then converted to HTML/CSS/JS using the mkdocs static site generator. We have customized the mkdocs-material theme to align with NCAR branding and colors. Making Contributions \u00b6 Simple Contributions \u00b6 If you're looking to make a minor adjustment to our documentation, such as fixing a typo or adding minor enhancements to a few documents, GitHub's built-in web editor and web IDE make it easy. As you browse our documentation, you'll notice a pencil icon next to the header on each page. This icon is a shortcut to edit the current page. Here's how you can use this feature: Click on the pencil icon to open the editor. Make your desired changes. After you've made your changes, be sure to update the commit message. A clear and concise commit message helps us understand your contribution better. While not mandatory, we recommend renaming the branch for better organization and tracking of changes. Submit your changes. No contribution is too small. We appreciate your help in improving and maintaining the quality of our documentation. Visit out guidelines for writing better documentations. Complex Contributions \u00b6 For more substantial modifications, such as a comprehensive revision of a section in the documentation, we recommend cloning the repository to your local machine and working from there. Prerequisites \u00b6 Before you start, make sure you have the following installed on your local machine: Git: This is used for source code management and version control. Python: Since MkDocs is a Python project, you'll need Python installed to work with it. A text editor or IDE of your choice. Steps to Contribute \u00b6 Fork the repository: Go to the repository page and click the \"Fork\" button to create a copy of the repository in your GitHub account. Clone the forked repository to your local machine: This can be done by running the following command in your terminal: git clone https://github.com/<YOUR_USERNAME>/hpc-docs-demo.git Replace <YOUR_USERNAME> with your GitHub username. Create a new branch: It's a good practice to create a new branch before you start making changes. This can be done by running: git checkout -b <BRANCH_NAME> Replace <BRANCH_NAME> with a name that gives a hint about the changes you're about to make. Create an Environment: To build the documentation locally, you'll need to install certain dependencies. Although this step is optional, we strongly recommend it. The example provided here utilizes a conda environment, but feel free to use any Python 3 environment of your preference. conda create -n docs pip conda activate docs pip install -r requirements.txt Make your changes: With your new branch checked out, you can start making changes to the documentation. Remember to save your work regularly. Test your changes: Make sure that your changes do not introduce any errors. With MkDocs, you can preview your changes by running mkdocs serve in your terminal. This starts a local server where you can preview your work. Commit your changes: Once you have made and tested your changes, stage the files you have modified using git add <file> or git add . to stage all changes. Then, commit your changes with a descriptive message using git commit -m \"<YOUR_COMMIT_MESSAGE>\" . Push your changes: You can push your changes to your forked repository by running git push origin <BRANCH_NAME> . Submit a Pull Request (PR): After pushing your changes, go to your forked repository on GitHub, and click on \"New pull request\". Fill in the necessary details and submit the PR. Please note, for larger changes, it's often a good idea to discuss your plans in an issue before investing a lot of time in implementation. This helps to ensure that your efforts align with the project's direction and your changes have a higher chance of being accepted. Feedback and Support \u00b6 If you have any questions or need assistance while contributing to this repository, please reach out to the repository maintainers or open an issue on the GitHub repository page. Thank you for your contributions and helping us create a valuable resource for NCAR's HPC community!","title":"About"},{"location":"contributing/#contribution-guide","text":"Welcome to the NCAR HPC Resources GitHub repository! This guide provides an overview of how to contribute to this project and the standards to follow when adding content to this repository. Our goal is to create a comprehensive and user-friendly documentation resource for NCAR's HPC resources.","title":"Contribution Guide"},{"location":"contributing/#repository-overview","text":"This repository contains technical documentation for NCAR HPC resources. The documentation is written in Markdown, which is then converted to HTML/CSS/JS using the mkdocs static site generator. We have customized the mkdocs-material theme to align with NCAR branding and colors.","title":"Repository Overview"},{"location":"contributing/#making-contributions","text":"","title":"Making Contributions"},{"location":"contributing/#simple-contributions","text":"If you're looking to make a minor adjustment to our documentation, such as fixing a typo or adding minor enhancements to a few documents, GitHub's built-in web editor and web IDE make it easy. As you browse our documentation, you'll notice a pencil icon next to the header on each page. This icon is a shortcut to edit the current page. Here's how you can use this feature: Click on the pencil icon to open the editor. Make your desired changes. After you've made your changes, be sure to update the commit message. A clear and concise commit message helps us understand your contribution better. While not mandatory, we recommend renaming the branch for better organization and tracking of changes. Submit your changes. No contribution is too small. We appreciate your help in improving and maintaining the quality of our documentation. Visit out guidelines for writing better documentations.","title":"Simple Contributions"},{"location":"contributing/#complex-contributions","text":"For more substantial modifications, such as a comprehensive revision of a section in the documentation, we recommend cloning the repository to your local machine and working from there.","title":"Complex Contributions"},{"location":"contributing/#prerequisites","text":"Before you start, make sure you have the following installed on your local machine: Git: This is used for source code management and version control. Python: Since MkDocs is a Python project, you'll need Python installed to work with it. A text editor or IDE of your choice.","title":"Prerequisites"},{"location":"contributing/#steps-to-contribute","text":"Fork the repository: Go to the repository page and click the \"Fork\" button to create a copy of the repository in your GitHub account. Clone the forked repository to your local machine: This can be done by running the following command in your terminal: git clone https://github.com/<YOUR_USERNAME>/hpc-docs-demo.git Replace <YOUR_USERNAME> with your GitHub username. Create a new branch: It's a good practice to create a new branch before you start making changes. This can be done by running: git checkout -b <BRANCH_NAME> Replace <BRANCH_NAME> with a name that gives a hint about the changes you're about to make. Create an Environment: To build the documentation locally, you'll need to install certain dependencies. Although this step is optional, we strongly recommend it. The example provided here utilizes a conda environment, but feel free to use any Python 3 environment of your preference. conda create -n docs pip conda activate docs pip install -r requirements.txt Make your changes: With your new branch checked out, you can start making changes to the documentation. Remember to save your work regularly. Test your changes: Make sure that your changes do not introduce any errors. With MkDocs, you can preview your changes by running mkdocs serve in your terminal. This starts a local server where you can preview your work. Commit your changes: Once you have made and tested your changes, stage the files you have modified using git add <file> or git add . to stage all changes. Then, commit your changes with a descriptive message using git commit -m \"<YOUR_COMMIT_MESSAGE>\" . Push your changes: You can push your changes to your forked repository by running git push origin <BRANCH_NAME> . Submit a Pull Request (PR): After pushing your changes, go to your forked repository on GitHub, and click on \"New pull request\". Fill in the necessary details and submit the PR. Please note, for larger changes, it's often a good idea to discuss your plans in an issue before investing a lot of time in implementation. This helps to ensure that your efforts align with the project's direction and your changes have a higher chance of being accepted.","title":"Steps to Contribute"},{"location":"contributing/#feedback-and-support","text":"If you have any questions or need assistance while contributing to this repository, please reach out to the repository maintainers or open an issue on the GitHub repository page. Thank you for your contributions and helping us create a valuable resource for NCAR's HPC community!","title":"Feedback and Support"},{"location":"getting-started/","text":"Getting started with NCAR HPC Resources \u00b6 About this page This document will guide you through the basics of using NCAR's supercomputers, storage systems, and services. Once you are authorized to use NCAR compute and storage resources, and you have an account and the necessary software , you can follow the procedures described below to log in. These pages provide information on compiling your code, submitting jobs, and performing other common tasks on all NCAR resources unless otherwise noted: Compiling Code Starting and Managing Jobs with PBS Managing Your Allocation New User's Resources \u00b6 New User Orientation New User Training for HPC Systems Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators. Logging In \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. This page is just a copy of https://arc.ucar.edu/knowledge_base/87655186 in mkdown for demonstrations.","title":"Getting started with NCAR HPC Resources"},{"location":"getting-started/#getting-started-with-ncar-hpc-resources","text":"About this page This document will guide you through the basics of using NCAR's supercomputers, storage systems, and services. Once you are authorized to use NCAR compute and storage resources, and you have an account and the necessary software , you can follow the procedures described below to log in. These pages provide information on compiling your code, submitting jobs, and performing other common tasks on all NCAR resources unless otherwise noted: Compiling Code Starting and Managing Jobs with PBS Managing Your Allocation","title":"Getting started with NCAR HPC Resources"},{"location":"getting-started/#new-users-resources","text":"New User Orientation New User Training for HPC Systems Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.","title":"New User's Resources"},{"location":"getting-started/#logging-in","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. This page is just a copy of https://arc.ucar.edu/knowledge_base/87655186 in mkdown for demonstrations.","title":"Logging In"},{"location":"user-support/","text":"User Support \u00b6 Documentation \u00b6 Search for relevant documentation with the \"Search\" field at upper-right or use one of the service desk links below to submit a help request (login required). Where a login is required, use your NCAR/UCAR username and CIT password. Users who do not have CIT passwords (non-NCAR/UCAR staff members, for example) can reset their passwords here or call 303-497-2400 for assistance. NCAR Research Computing help desk \u00b6 Contact the help desk for assistance with Cheyenne, Casper, and related data-storage systems. Enterprise Service Desk staff support \u00b6 The Enterprise Service Desk portal is for NCAR/UCAR staff who need support for internal IT services.","title":"User Support"},{"location":"user-support/#user-support","text":"","title":"User Support"},{"location":"user-support/#documentation","text":"Search for relevant documentation with the \"Search\" field at upper-right or use one of the service desk links below to submit a help request (login required). Where a login is required, use your NCAR/UCAR username and CIT password. Users who do not have CIT passwords (non-NCAR/UCAR staff members, for example) can reset their passwords here or call 303-497-2400 for assistance.","title":"Documentation"},{"location":"user-support/#ncar-research-computing-help-desk","text":"Contact the help desk for assistance with Cheyenne, Casper, and related data-storage systems.","title":"NCAR Research Computing help desk"},{"location":"user-support/#enterprise-service-desk-staff-support","text":"The Enterprise Service Desk portal is for NCAR/UCAR staff who need support for internal IT services.","title":"Enterprise Service Desk staff support"},{"location":"allocations/","text":"Allocations \u00b6 The Computational and Information Systems Laboratory (CISL) provides large-scale computing resources for university researchers and NCAR scientists in the atmospheric and related sciences. To access these supercomputers, storage systems, and other resources, users must apply for allocations via the processes defined for each community. Applications are reviewed and time is allocated according to the needs of the projects and the availability of resources. Send questions about the following allocation opportunities to alloc@ucar.edu . Info The next university deadline for submitting Large Allocation Requests is March 13, 2023. Page contents \u00b6 Allocations Page contents University allocation opportunities Wyoming-NCAR Alliance NCAR lab allocation opportunities University allocation opportunities \u00b6 NCAR provides computing resources to the university community for investigations that are beyond the scope of university computing centers. The CISL HPC Advisory Panel (CHAP) accepts requests for large allocations of NCAR resources every six months, in March and September. See University allocations for details. Info Eligibility. In general, any U.S.-based researcher with an NSF award in the atmospheric sciences or computational science in support of the atmospheric sciences is eligible to apply for a University Community allocation. There are some limited opportunities for those without NSF awards. Wyoming-NCAR Alliance \u00b6 The NCAR-Wyoming Supercomputing Center represents a collaboration between NCAR and the University of Wyoming. As part of the Wyoming-NCAR Alliance (WNA), a portion of the Cheyenne system \u2013 about 160 million core-hours per year \u2013 is reserved for Wyoming-led projects and allocated by a University of Wyoming-managed process. Details of the Wyoming process are available at the University of Wyoming web site . NCAR lab allocation opportunities \u00b6 NCAR investigators have access to CISL resources through allocations to the NCAR labs and have opportunities to submit requests for larger-scale, project-oriented allocations. Proposals for larger-scale projects are reviewed twice per year to become NCAR Strategic Capability projects. See NCAR allocations for more details. Just a copy of https://arc.ucar.edu/knowledge_base/74317835 .","title":"Allocations"},{"location":"allocations/#allocations","text":"The Computational and Information Systems Laboratory (CISL) provides large-scale computing resources for university researchers and NCAR scientists in the atmospheric and related sciences. To access these supercomputers, storage systems, and other resources, users must apply for allocations via the processes defined for each community. Applications are reviewed and time is allocated according to the needs of the projects and the availability of resources. Send questions about the following allocation opportunities to alloc@ucar.edu . Info The next university deadline for submitting Large Allocation Requests is March 13, 2023.","title":"Allocations"},{"location":"allocations/#page-contents","text":"Allocations Page contents University allocation opportunities Wyoming-NCAR Alliance NCAR lab allocation opportunities","title":"Page contents"},{"location":"allocations/#university-allocation-opportunities","text":"NCAR provides computing resources to the university community for investigations that are beyond the scope of university computing centers. The CISL HPC Advisory Panel (CHAP) accepts requests for large allocations of NCAR resources every six months, in March and September. See University allocations for details. Info Eligibility. In general, any U.S.-based researcher with an NSF award in the atmospheric sciences or computational science in support of the atmospheric sciences is eligible to apply for a University Community allocation. There are some limited opportunities for those without NSF awards.","title":"University allocation opportunities"},{"location":"allocations/#wyoming-ncar-alliance","text":"The NCAR-Wyoming Supercomputing Center represents a collaboration between NCAR and the University of Wyoming. As part of the Wyoming-NCAR Alliance (WNA), a portion of the Cheyenne system \u2013 about 160 million core-hours per year \u2013 is reserved for Wyoming-led projects and allocated by a University of Wyoming-managed process. Details of the Wyoming process are available at the University of Wyoming web site .","title":"Wyoming-NCAR Alliance"},{"location":"allocations/#ncar-lab-allocation-opportunities","text":"NCAR investigators have access to CISL resources through allocations to the NCAR labs and have opportunities to submit requests for larger-scale, project-oriented allocations. Proposals for larger-scale projects are reviewed twice per year to become NCAR Strategic Capability projects. See NCAR allocations for more details. Just a copy of https://arc.ucar.edu/knowledge_base/74317835 .","title":"NCAR lab allocation opportunities"},{"location":"allocations/allocations/","text":"Allocations \u00b6 The Computational and Information Systems Laboratory (CISL) provides large-scale computing resources for university researchers and NCAR scientists in the atmospheric and related sciences. To access these supercomputers, storage systems, and other resources, users must apply for allocations via the processes defined for each community. Applications are reviewed and time is allocated according to the needs of the projects and the availability of resources. Send questions about the following allocation opportunities to alloc@ucar.edu . Info The next university deadline for submitting Large Allocation Requests is March 13, 2023. Page contents \u00b6 Allocations Page contents University allocation opportunities Wyoming-NCAR Alliance NCAR lab allocation opportunities University allocation opportunities \u00b6 NCAR provides computing resources to the university community for investigations that are beyond the scope of university computing centers. The CISL HPC Advisory Panel (CHAP) accepts requests for large allocations of NCAR resources every six months, in March and September. See University allocations for details. Info Eligibility. In general, any U.S.-based researcher with an NSF award in the atmospheric sciences or computational science in support of the atmospheric sciences is eligible to apply for a University Community allocation. There are some limited opportunities for those without NSF awards. Wyoming-NCAR Alliance \u00b6 The NCAR-Wyoming Supercomputing Center represents a collaboration between NCAR and the University of Wyoming. As part of the Wyoming-NCAR Alliance (WNA), a portion of the Cheyenne system \u2013 about 160 million core-hours per year \u2013 is reserved for Wyoming-led projects and allocated by a University of Wyoming-managed process. Details of the Wyoming process are available at the University of Wyoming web site . NCAR lab allocation opportunities \u00b6 NCAR investigators have access to CISL resources through allocations to the NCAR labs and have opportunities to submit requests for larger-scale, project-oriented allocations. Proposals for larger-scale projects are reviewed twice per year to become NCAR Strategic Capability projects. See NCAR allocations for more details. Just a copy of https://arc.ucar.edu/knowledge_base/74317835 .","title":"Allocations"},{"location":"allocations/allocations/#allocations","text":"The Computational and Information Systems Laboratory (CISL) provides large-scale computing resources for university researchers and NCAR scientists in the atmospheric and related sciences. To access these supercomputers, storage systems, and other resources, users must apply for allocations via the processes defined for each community. Applications are reviewed and time is allocated according to the needs of the projects and the availability of resources. Send questions about the following allocation opportunities to alloc@ucar.edu . Info The next university deadline for submitting Large Allocation Requests is March 13, 2023.","title":"Allocations"},{"location":"allocations/allocations/#page-contents","text":"Allocations Page contents University allocation opportunities Wyoming-NCAR Alliance NCAR lab allocation opportunities","title":"Page contents"},{"location":"allocations/allocations/#university-allocation-opportunities","text":"NCAR provides computing resources to the university community for investigations that are beyond the scope of university computing centers. The CISL HPC Advisory Panel (CHAP) accepts requests for large allocations of NCAR resources every six months, in March and September. See University allocations for details. Info Eligibility. In general, any U.S.-based researcher with an NSF award in the atmospheric sciences or computational science in support of the atmospheric sciences is eligible to apply for a University Community allocation. There are some limited opportunities for those without NSF awards.","title":"University allocation opportunities"},{"location":"allocations/allocations/#wyoming-ncar-alliance","text":"The NCAR-Wyoming Supercomputing Center represents a collaboration between NCAR and the University of Wyoming. As part of the Wyoming-NCAR Alliance (WNA), a portion of the Cheyenne system \u2013 about 160 million core-hours per year \u2013 is reserved for Wyoming-led projects and allocated by a University of Wyoming-managed process. Details of the Wyoming process are available at the University of Wyoming web site .","title":"Wyoming-NCAR Alliance"},{"location":"allocations/allocations/#ncar-lab-allocation-opportunities","text":"NCAR investigators have access to CISL resources through allocations to the NCAR labs and have opportunities to submit requests for larger-scale, project-oriented allocations. Proposals for larger-scale projects are reviewed twice per year to become NCAR Strategic Capability projects. See NCAR allocations for more details. Just a copy of https://arc.ucar.edu/knowledge_base/74317835 .","title":"NCAR lab allocation opportunities"},{"location":"allocations/ncar_allocations/","text":"NCAR allocations \u00b6 A new generation of NCAR supercomputing resources began when the Yellowstone system was installed at the NCAR-Wyoming Supercomputing Center (NWSC) in 2012. With the subsequent introduction of the Cheyenne HPC system in 2017, the capabilities and capacity of this resource environment expanded significantly. NCAR\u2019s portion of the Community Computing pool amounts to ~29% of these resources. For the Cheyenne system alone, this represents an estimated 350 million core-hours per year (compared to 170 million on Yellowstone and fewer than 9 million core-hours per year on Bluefire). Similar portions of the data analysis and visualization resources and GLADE storage system also are available to NCAR users. Page contents \u00b6 NCAR Strategic Capability (NSC) projects NCAR Director's Reserve NCAR Lab Grants NCAR and university use Allocation categories \u00b6 Three categories of allocations are available to NCAR users: NCAR Strategic Capability (NSC) Projects NCAR Director\u2019s Reserve NCAR Lab Grants Joint NCAR/university allocations have been discontinued, although projects with NCAR visitors or university-based collaborators are eligible to use or make requests for all three categories of NCAR allocations. See NCAR and university use below. NCAR Strategic Capability (NSC) projects \u00b6 After review of scientific merit, strategic importance, technical readiness, and broader impact, these large-scale, high-priority projects receive 17% of the core-hours available to NCAR. See the NSC Projects page for details, including the next deadline for submitting requests. NCAR Director's Reserve \u00b6 The NCAR Director\u2019s Reserve comprises 2% of the available NCAR resources. Access is disbursed at the discretion of the NCAR director and is designed to accommodate work that does not fit within any other allocation mechanism (whether CSL, university, or NCAR). Director\u2019s Reserve requests must meet the following two criteria: The project should have clear relevance to the NCAR mission or strategic priorities. The project will be completed in less than one year and should not be an ongoing or recurring activity. Reserve requests also should also meet at least some of the following six criteria: The work or project lead does not meet the eligibility criteria for other allocation mechanisms (for example, the work is led by collaborators at or supported by non-NSF agencies or labs). The project has come up unexpectedly and has an urgency that cannot be accommodated by other allocation mechanisms (for example, simulations related to an ongoing wildfire, oil spill, or storm). The project is supported by a funding agency award separate from NCAR Base funding that was awarded at a time incompatible with the NCAR lab or NSC allocation timelines. The work has tangible benefits to NCAR as a whole that do not serve as valued criteria for other allocation opportunities (for example, educational, public outreach/service, political). The work cannot be accommodated within the relevant NCAR Lab Grant but has the backing of the NCAR lab leadership. \u201cMatching\u201d allocations are encouraged. That is, the Director\u2019s Reserve allocation will be matched by support from an NCAR Lab Grant. The work involves collaborators from multiple NCAR labs or multiple organizations or institutions outside of NCAR. (Note that a request that meets only this criterion is unlikely to merit a Director\u2019s Reserve allocation; typically, other criteria also must be met.) Projects that may be suitable for an NSC allocation but that cannot wait until the next NSC round can request a startup allocation from the Director\u2019s Reserve; such requests still must explain why they satisfy the criteria for a Director\u2019s Reserve allocation. To request a Director\u2019s Reserve allocation, the NCAR Lab Allocation administrator should submit a brief write-up (approximately one page) from the prospective project lead that describes the project to be conducted and its computing requirements. The NCAR Lab Allocation administrator should include a statement describing why the work should be considered for a Director\u2019s Reserve allocation. The request should be submitted to alloc@ucar.edu . Director\u2019s Reserve requests are reviewed as they are submitted, and decisions generally are made within a few days. Reporting \u00b6 Director\u2019s Reserve allocations come with commensurate reporting requirements that vary depending on the size of the request and award. At the end of each project, the project lead will document the work conducted, resulting outcomes, and contributions toward the strategic priority. For all projects, the project lead should prepare a short write-up, usually less than one page. Larger projects also may be asked to produce, in addition to the short write-up, a brief (15-minute) presentation to the Executive Committee. This reporting requirement will be identified at the time of the award. NCAR Lab Grants \u00b6 NCAR labs receive 10% of the HPC systems\u2019 available core-hours. The allocations are assumed to remain at the same levels while a system is in production. However, a member of the NCAR Executive Committee (EC) can request that the lab allocations be reviewed and potentially revised or adjusted due to changes in lab needs or priorities. The EC member should initiate this process at an EC meeting, which will determine the timeline and process for considering the request. Within the lab-level \u201cblocks,\u201d the labs allocate resources according to their strategic priorities. They are expected to accommodate small- to medium-sized activities within these locally managed allocations, including joint work with collaborators, regularly scheduled workshops and training activities, preparatory work for larger-scale projects, and work by visiting, postdoctoral, or graduate student researchers. (NCAR short-term visitors also may apply for university allocations if they meet all necessary eligibility requirements.) The only annual reporting requirements for lab allocations are acknowledgement of CISL resources in relevant publications and in the lab\u2019s annual report, and citations for those publications to be sent to CISL. In addition to 10% of the Cheyenne HPC resource, the NCAR labs receive allocations of a similar fraction of analysis and visualization resource use and GLADE project space. Storage space requests and allocations are constrained by the growth supported by CISL budget for system expansion. Within these constraints, NCAR labs and staff will have to make trade-offs and data management decisions, especially when considering storage of data that are generated on resources outside of CISL. NCAR and university use \u00b6 The NWSC resources are shared among several allocation \u201cfacilities,\u201d including the NCAR Community and University Community in addition to the Climate Simulation Laboratory and the Wyoming Community. As in the past, the NCAR and university communities each get an equal portion of the resources, and NCAR and CISL are responsible for maintaining that balance. To support this NCAR/University balance, we offer the following guidelines for appropriate use of the NCAR and university resource pools. NCAR visitors (visiting scientists, post-docs, and so on) who are not permanent UCAR staff are eligible to apply for university allocations, subject to the eligibility policies for university allocations. UCAR/UCP (that is, non-NCAR) permanent staff may request university allocations, subject to the university eligibility policies. NCAR Labs and NSC projects may choose to allow visitors and collaborators to use those allocations as part of collaborative projects. For joint university/NCAR work in which permanent NCAR staff will be responsible for a significant amount of computational usage or a significant fraction of the project\u2019s total computational work, the preferred approach is for NCAR staff to request the necessary resources from the NCAR pool, while the university researchers request the necessary resources for their activities from the university pool. A university principal investigator with a university allocation in support of an NSF award may elect to permit an NCAR collaborator on that award to access an incidental amount of the awarded allocation in support of the collaboration. NCAR policies and guidelines for co-sponsorship are not affected by these revised allocation policies. Co-sponsorship remains a transaction between a lab and the proposer, and the process is monitored by UCAR Budget and Planning and PACUR. The UCAR B&P office has approved rates for use in the proposal process. Just a copy of https://arc.ucar.edu/knowledge_base/75694354","title":"NCAR allocations"},{"location":"allocations/ncar_allocations/#ncar-allocations","text":"A new generation of NCAR supercomputing resources began when the Yellowstone system was installed at the NCAR-Wyoming Supercomputing Center (NWSC) in 2012. With the subsequent introduction of the Cheyenne HPC system in 2017, the capabilities and capacity of this resource environment expanded significantly. NCAR\u2019s portion of the Community Computing pool amounts to ~29% of these resources. For the Cheyenne system alone, this represents an estimated 350 million core-hours per year (compared to 170 million on Yellowstone and fewer than 9 million core-hours per year on Bluefire). Similar portions of the data analysis and visualization resources and GLADE storage system also are available to NCAR users.","title":"NCAR allocations"},{"location":"allocations/ncar_allocations/#page-contents","text":"NCAR Strategic Capability (NSC) projects NCAR Director's Reserve NCAR Lab Grants NCAR and university use","title":"Page contents"},{"location":"allocations/ncar_allocations/#allocation-categories","text":"Three categories of allocations are available to NCAR users: NCAR Strategic Capability (NSC) Projects NCAR Director\u2019s Reserve NCAR Lab Grants Joint NCAR/university allocations have been discontinued, although projects with NCAR visitors or university-based collaborators are eligible to use or make requests for all three categories of NCAR allocations. See NCAR and university use below.","title":"Allocation categories"},{"location":"allocations/ncar_allocations/#ncar-strategic-capability-nsc-projects","text":"After review of scientific merit, strategic importance, technical readiness, and broader impact, these large-scale, high-priority projects receive 17% of the core-hours available to NCAR. See the NSC Projects page for details, including the next deadline for submitting requests.","title":"NCAR Strategic Capability (NSC) projects"},{"location":"allocations/ncar_allocations/#ncar-directors-reserve","text":"The NCAR Director\u2019s Reserve comprises 2% of the available NCAR resources. Access is disbursed at the discretion of the NCAR director and is designed to accommodate work that does not fit within any other allocation mechanism (whether CSL, university, or NCAR). Director\u2019s Reserve requests must meet the following two criteria: The project should have clear relevance to the NCAR mission or strategic priorities. The project will be completed in less than one year and should not be an ongoing or recurring activity. Reserve requests also should also meet at least some of the following six criteria: The work or project lead does not meet the eligibility criteria for other allocation mechanisms (for example, the work is led by collaborators at or supported by non-NSF agencies or labs). The project has come up unexpectedly and has an urgency that cannot be accommodated by other allocation mechanisms (for example, simulations related to an ongoing wildfire, oil spill, or storm). The project is supported by a funding agency award separate from NCAR Base funding that was awarded at a time incompatible with the NCAR lab or NSC allocation timelines. The work has tangible benefits to NCAR as a whole that do not serve as valued criteria for other allocation opportunities (for example, educational, public outreach/service, political). The work cannot be accommodated within the relevant NCAR Lab Grant but has the backing of the NCAR lab leadership. \u201cMatching\u201d allocations are encouraged. That is, the Director\u2019s Reserve allocation will be matched by support from an NCAR Lab Grant. The work involves collaborators from multiple NCAR labs or multiple organizations or institutions outside of NCAR. (Note that a request that meets only this criterion is unlikely to merit a Director\u2019s Reserve allocation; typically, other criteria also must be met.) Projects that may be suitable for an NSC allocation but that cannot wait until the next NSC round can request a startup allocation from the Director\u2019s Reserve; such requests still must explain why they satisfy the criteria for a Director\u2019s Reserve allocation. To request a Director\u2019s Reserve allocation, the NCAR Lab Allocation administrator should submit a brief write-up (approximately one page) from the prospective project lead that describes the project to be conducted and its computing requirements. The NCAR Lab Allocation administrator should include a statement describing why the work should be considered for a Director\u2019s Reserve allocation. The request should be submitted to alloc@ucar.edu . Director\u2019s Reserve requests are reviewed as they are submitted, and decisions generally are made within a few days.","title":"NCAR Director's Reserve"},{"location":"allocations/ncar_allocations/#reporting","text":"Director\u2019s Reserve allocations come with commensurate reporting requirements that vary depending on the size of the request and award. At the end of each project, the project lead will document the work conducted, resulting outcomes, and contributions toward the strategic priority. For all projects, the project lead should prepare a short write-up, usually less than one page. Larger projects also may be asked to produce, in addition to the short write-up, a brief (15-minute) presentation to the Executive Committee. This reporting requirement will be identified at the time of the award.","title":"Reporting"},{"location":"allocations/ncar_allocations/#ncar-lab-grants","text":"NCAR labs receive 10% of the HPC systems\u2019 available core-hours. The allocations are assumed to remain at the same levels while a system is in production. However, a member of the NCAR Executive Committee (EC) can request that the lab allocations be reviewed and potentially revised or adjusted due to changes in lab needs or priorities. The EC member should initiate this process at an EC meeting, which will determine the timeline and process for considering the request. Within the lab-level \u201cblocks,\u201d the labs allocate resources according to their strategic priorities. They are expected to accommodate small- to medium-sized activities within these locally managed allocations, including joint work with collaborators, regularly scheduled workshops and training activities, preparatory work for larger-scale projects, and work by visiting, postdoctoral, or graduate student researchers. (NCAR short-term visitors also may apply for university allocations if they meet all necessary eligibility requirements.) The only annual reporting requirements for lab allocations are acknowledgement of CISL resources in relevant publications and in the lab\u2019s annual report, and citations for those publications to be sent to CISL. In addition to 10% of the Cheyenne HPC resource, the NCAR labs receive allocations of a similar fraction of analysis and visualization resource use and GLADE project space. Storage space requests and allocations are constrained by the growth supported by CISL budget for system expansion. Within these constraints, NCAR labs and staff will have to make trade-offs and data management decisions, especially when considering storage of data that are generated on resources outside of CISL.","title":"NCAR Lab Grants"},{"location":"allocations/ncar_allocations/#ncar-and-university-use","text":"The NWSC resources are shared among several allocation \u201cfacilities,\u201d including the NCAR Community and University Community in addition to the Climate Simulation Laboratory and the Wyoming Community. As in the past, the NCAR and university communities each get an equal portion of the resources, and NCAR and CISL are responsible for maintaining that balance. To support this NCAR/University balance, we offer the following guidelines for appropriate use of the NCAR and university resource pools. NCAR visitors (visiting scientists, post-docs, and so on) who are not permanent UCAR staff are eligible to apply for university allocations, subject to the eligibility policies for university allocations. UCAR/UCP (that is, non-NCAR) permanent staff may request university allocations, subject to the university eligibility policies. NCAR Labs and NSC projects may choose to allow visitors and collaborators to use those allocations as part of collaborative projects. For joint university/NCAR work in which permanent NCAR staff will be responsible for a significant amount of computational usage or a significant fraction of the project\u2019s total computational work, the preferred approach is for NCAR staff to request the necessary resources from the NCAR pool, while the university researchers request the necessary resources for their activities from the university pool. A university principal investigator with a university allocation in support of an NSF award may elect to permit an NCAR collaborator on that award to access an incidental amount of the awarded allocation in support of the collaboration. NCAR policies and guidelines for co-sponsorship are not affected by these revised allocation policies. Co-sponsorship remains a transaction between a lab and the proposer, and the process is monitored by UCAR Budget and Planning and PACUR. The UCAR B&P office has approved rates for use in the proposal process. Just a copy of https://arc.ucar.edu/knowledge_base/75694354","title":"NCAR and university use"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/","text":"NCAR allocations \u00b6 A new generation of NCAR supercomputing resources began when the Yellowstone system was installed at the NCAR-Wyoming Supercomputing Center (NWSC) in 2012. With the subsequent introduction of the Cheyenne HPC system in 2017, the capabilities and capacity of this resource environment expanded significantly. NCAR\u2019s portion of the Community Computing pool amounts to ~29% of these resources. For the Cheyenne system alone, this represents an estimated 350 million core-hours per year (compared to 170 million on Yellowstone and fewer than 9 million core-hours per year on Bluefire). Similar portions of the data analysis and visualization resources and GLADE storage system also are available to NCAR users. Page contents \u00b6 NCAR Strategic Capability (NSC) projects NCAR Director's Reserve NCAR Lab Grants NCAR and university use Allocation categories \u00b6 Three categories of allocations are available to NCAR users: NCAR Strategic Capability (NSC) Projects NCAR Director\u2019s Reserve NCAR Lab Grants Joint NCAR/university allocations have been discontinued, although projects with NCAR visitors or university-based collaborators are eligible to use or make requests for all three categories of NCAR allocations. See NCAR and university use below. NCAR Strategic Capability (NSC) projects \u00b6 After review of scientific merit, strategic importance, technical readiness, and broader impact, these large-scale, high-priority projects receive 17% of the core-hours available to NCAR. See the NSC Projects page for details, including the next deadline for submitting requests. NCAR Director's Reserve \u00b6 The NCAR Director\u2019s Reserve comprises 2% of the available NCAR resources. Access is disbursed at the discretion of the NCAR director and is designed to accommodate work that does not fit within any other allocation mechanism (whether CSL, university, or NCAR). Director\u2019s Reserve requests must meet the following two criteria: The project should have clear relevance to the NCAR mission or strategic priorities. The project will be completed in less than one year and should not be an ongoing or recurring activity. Reserve requests also should also meet at least some of the following six criteria: The work or project lead does not meet the eligibility criteria for other allocation mechanisms (for example, the work is led by collaborators at or supported by non-NSF agencies or labs). The project has come up unexpectedly and has an urgency that cannot be accommodated by other allocation mechanisms (for example, simulations related to an ongoing wildfire, oil spill, or storm). The project is supported by a funding agency award separate from NCAR Base funding that was awarded at a time incompatible with the NCAR lab or NSC allocation timelines. The work has tangible benefits to NCAR as a whole that do not serve as valued criteria for other allocation opportunities (for example, educational, public outreach/service, political). The work cannot be accommodated within the relevant NCAR Lab Grant but has the backing of the NCAR lab leadership. \u201cMatching\u201d allocations are encouraged. That is, the Director\u2019s Reserve allocation will be matched by support from an NCAR Lab Grant. The work involves collaborators from multiple NCAR labs or multiple organizations or institutions outside of NCAR. (Note that a request that meets only this criterion is unlikely to merit a Director\u2019s Reserve allocation; typically, other criteria also must be met.) Projects that may be suitable for an NSC allocation but that cannot wait until the next NSC round can request a startup allocation from the Director\u2019s Reserve; such requests still must explain why they satisfy the criteria for a Director\u2019s Reserve allocation. To request a Director\u2019s Reserve allocation, the NCAR Lab Allocation administrator should submit a brief write-up (approximately one page) from the prospective project lead that describes the project to be conducted and its computing requirements. The NCAR Lab Allocation administrator should include a statement describing why the work should be considered for a Director\u2019s Reserve allocation. The request should be submitted to alloc@ucar.edu . Director\u2019s Reserve requests are reviewed as they are submitted, and decisions generally are made within a few days. Reporting \u00b6 Director\u2019s Reserve allocations come with commensurate reporting requirements that vary depending on the size of the request and award. At the end of each project, the project lead will document the work conducted, resulting outcomes, and contributions toward the strategic priority. For all projects, the project lead should prepare a short write-up, usually less than one page. Larger projects also may be asked to produce, in addition to the short write-up, a brief (15-minute) presentation to the Executive Committee. This reporting requirement will be identified at the time of the award. NCAR Lab Grants \u00b6 NCAR labs receive 10% of the HPC systems\u2019 available core-hours. The allocations are assumed to remain at the same levels while a system is in production. However, a member of the NCAR Executive Committee (EC) can request that the lab allocations be reviewed and potentially revised or adjusted due to changes in lab needs or priorities. The EC member should initiate this process at an EC meeting, which will determine the timeline and process for considering the request. Within the lab-level \u201cblocks,\u201d the labs allocate resources according to their strategic priorities. They are expected to accommodate small- to medium-sized activities within these locally managed allocations, including joint work with collaborators, regularly scheduled workshops and training activities, preparatory work for larger-scale projects, and work by visiting, postdoctoral, or graduate student researchers. (NCAR short-term visitors also may apply for university allocations if they meet all necessary eligibility requirements.) The only annual reporting requirements for lab allocations are acknowledgement of CISL resources in relevant publications and in the lab\u2019s annual report, and citations for those publications to be sent to CISL. In addition to 10% of the Cheyenne HPC resource, the NCAR labs receive allocations of a similar fraction of analysis and visualization resource use and GLADE project space. Storage space requests and allocations are constrained by the growth supported by CISL budget for system expansion. Within these constraints, NCAR labs and staff will have to make trade-offs and data management decisions, especially when considering storage of data that are generated on resources outside of CISL. NCAR and university use \u00b6 The NWSC resources are shared among several allocation \u201cfacilities,\u201d including the NCAR Community and University Community in addition to the Climate Simulation Laboratory and the Wyoming Community. As in the past, the NCAR and university communities each get an equal portion of the resources, and NCAR and CISL are responsible for maintaining that balance. To support this NCAR/University balance, we offer the following guidelines for appropriate use of the NCAR and university resource pools. NCAR visitors (visiting scientists, post-docs, and so on) who are not permanent UCAR staff are eligible to apply for university allocations, subject to the eligibility policies for university allocations. UCAR/UCP (that is, non-NCAR) permanent staff may request university allocations, subject to the university eligibility policies. NCAR Labs and NSC projects may choose to allow visitors and collaborators to use those allocations as part of collaborative projects. For joint university/NCAR work in which permanent NCAR staff will be responsible for a significant amount of computational usage or a significant fraction of the project\u2019s total computational work, the preferred approach is for NCAR staff to request the necessary resources from the NCAR pool, while the university researchers request the necessary resources for their activities from the university pool. A university principal investigator with a university allocation in support of an NSF award may elect to permit an NCAR collaborator on that award to access an incidental amount of the awarded allocation in support of the collaboration. NCAR policies and guidelines for co-sponsorship are not affected by these revised allocation policies. Co-sponsorship remains a transaction between a lab and the proposer, and the process is monitored by UCAR Budget and Planning and PACUR. The UCAR B&P office has approved rates for use in the proposal process. Just a copy of https://arc.ucar.edu/knowledge_base/75694354","title":"NCAR allocations"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#ncar-allocations","text":"A new generation of NCAR supercomputing resources began when the Yellowstone system was installed at the NCAR-Wyoming Supercomputing Center (NWSC) in 2012. With the subsequent introduction of the Cheyenne HPC system in 2017, the capabilities and capacity of this resource environment expanded significantly. NCAR\u2019s portion of the Community Computing pool amounts to ~29% of these resources. For the Cheyenne system alone, this represents an estimated 350 million core-hours per year (compared to 170 million on Yellowstone and fewer than 9 million core-hours per year on Bluefire). Similar portions of the data analysis and visualization resources and GLADE storage system also are available to NCAR users.","title":"NCAR allocations"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#page-contents","text":"NCAR Strategic Capability (NSC) projects NCAR Director's Reserve NCAR Lab Grants NCAR and university use","title":"Page contents"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#allocation-categories","text":"Three categories of allocations are available to NCAR users: NCAR Strategic Capability (NSC) Projects NCAR Director\u2019s Reserve NCAR Lab Grants Joint NCAR/university allocations have been discontinued, although projects with NCAR visitors or university-based collaborators are eligible to use or make requests for all three categories of NCAR allocations. See NCAR and university use below.","title":"Allocation categories"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#ncar-strategic-capability-nsc-projects","text":"After review of scientific merit, strategic importance, technical readiness, and broader impact, these large-scale, high-priority projects receive 17% of the core-hours available to NCAR. See the NSC Projects page for details, including the next deadline for submitting requests.","title":"NCAR Strategic Capability (NSC) projects"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#ncar-directors-reserve","text":"The NCAR Director\u2019s Reserve comprises 2% of the available NCAR resources. Access is disbursed at the discretion of the NCAR director and is designed to accommodate work that does not fit within any other allocation mechanism (whether CSL, university, or NCAR). Director\u2019s Reserve requests must meet the following two criteria: The project should have clear relevance to the NCAR mission or strategic priorities. The project will be completed in less than one year and should not be an ongoing or recurring activity. Reserve requests also should also meet at least some of the following six criteria: The work or project lead does not meet the eligibility criteria for other allocation mechanisms (for example, the work is led by collaborators at or supported by non-NSF agencies or labs). The project has come up unexpectedly and has an urgency that cannot be accommodated by other allocation mechanisms (for example, simulations related to an ongoing wildfire, oil spill, or storm). The project is supported by a funding agency award separate from NCAR Base funding that was awarded at a time incompatible with the NCAR lab or NSC allocation timelines. The work has tangible benefits to NCAR as a whole that do not serve as valued criteria for other allocation opportunities (for example, educational, public outreach/service, political). The work cannot be accommodated within the relevant NCAR Lab Grant but has the backing of the NCAR lab leadership. \u201cMatching\u201d allocations are encouraged. That is, the Director\u2019s Reserve allocation will be matched by support from an NCAR Lab Grant. The work involves collaborators from multiple NCAR labs or multiple organizations or institutions outside of NCAR. (Note that a request that meets only this criterion is unlikely to merit a Director\u2019s Reserve allocation; typically, other criteria also must be met.) Projects that may be suitable for an NSC allocation but that cannot wait until the next NSC round can request a startup allocation from the Director\u2019s Reserve; such requests still must explain why they satisfy the criteria for a Director\u2019s Reserve allocation. To request a Director\u2019s Reserve allocation, the NCAR Lab Allocation administrator should submit a brief write-up (approximately one page) from the prospective project lead that describes the project to be conducted and its computing requirements. The NCAR Lab Allocation administrator should include a statement describing why the work should be considered for a Director\u2019s Reserve allocation. The request should be submitted to alloc@ucar.edu . Director\u2019s Reserve requests are reviewed as they are submitted, and decisions generally are made within a few days.","title":"NCAR Director's Reserve"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#reporting","text":"Director\u2019s Reserve allocations come with commensurate reporting requirements that vary depending on the size of the request and award. At the end of each project, the project lead will document the work conducted, resulting outcomes, and contributions toward the strategic priority. For all projects, the project lead should prepare a short write-up, usually less than one page. Larger projects also may be asked to produce, in addition to the short write-up, a brief (15-minute) presentation to the Executive Committee. This reporting requirement will be identified at the time of the award.","title":"Reporting"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#ncar-lab-grants","text":"NCAR labs receive 10% of the HPC systems\u2019 available core-hours. The allocations are assumed to remain at the same levels while a system is in production. However, a member of the NCAR Executive Committee (EC) can request that the lab allocations be reviewed and potentially revised or adjusted due to changes in lab needs or priorities. The EC member should initiate this process at an EC meeting, which will determine the timeline and process for considering the request. Within the lab-level \u201cblocks,\u201d the labs allocate resources according to their strategic priorities. They are expected to accommodate small- to medium-sized activities within these locally managed allocations, including joint work with collaborators, regularly scheduled workshops and training activities, preparatory work for larger-scale projects, and work by visiting, postdoctoral, or graduate student researchers. (NCAR short-term visitors also may apply for university allocations if they meet all necessary eligibility requirements.) The only annual reporting requirements for lab allocations are acknowledgement of CISL resources in relevant publications and in the lab\u2019s annual report, and citations for those publications to be sent to CISL. In addition to 10% of the Cheyenne HPC resource, the NCAR labs receive allocations of a similar fraction of analysis and visualization resource use and GLADE project space. Storage space requests and allocations are constrained by the growth supported by CISL budget for system expansion. Within these constraints, NCAR labs and staff will have to make trade-offs and data management decisions, especially when considering storage of data that are generated on resources outside of CISL.","title":"NCAR Lab Grants"},{"location":"allocations/ncar_allocations/NCAR%2Ballocations/#ncar-and-university-use","text":"The NWSC resources are shared among several allocation \u201cfacilities,\u201d including the NCAR Community and University Community in addition to the Climate Simulation Laboratory and the Wyoming Community. As in the past, the NCAR and university communities each get an equal portion of the resources, and NCAR and CISL are responsible for maintaining that balance. To support this NCAR/University balance, we offer the following guidelines for appropriate use of the NCAR and university resource pools. NCAR visitors (visiting scientists, post-docs, and so on) who are not permanent UCAR staff are eligible to apply for university allocations, subject to the eligibility policies for university allocations. UCAR/UCP (that is, non-NCAR) permanent staff may request university allocations, subject to the university eligibility policies. NCAR Labs and NSC projects may choose to allow visitors and collaborators to use those allocations as part of collaborative projects. For joint university/NCAR work in which permanent NCAR staff will be responsible for a significant amount of computational usage or a significant fraction of the project\u2019s total computational work, the preferred approach is for NCAR staff to request the necessary resources from the NCAR pool, while the university researchers request the necessary resources for their activities from the university pool. A university principal investigator with a university allocation in support of an NSF award may elect to permit an NCAR collaborator on that award to access an incidental amount of the awarded allocation in support of the collaboration. NCAR policies and guidelines for co-sponsorship are not affected by these revised allocation policies. Co-sponsorship remains a transaction between a lab and the proposer, and the process is monitored by UCAR Budget and Planning and PACUR. The UCAR B&P office has approved rates for use in the proposal process. Just a copy of https://arc.ucar.edu/knowledge_base/75694354","title":"NCAR and university use"},{"location":"casper/","text":"Casper cluster \u00b6 The Casper cluster is a system of specialized data analysis and visualization resources; large-memory, multi-GPU nodes; and high-throughput computing nodes. Casper is composed of 100 nodes featuring Intel Skylake or Cascade Lake processors. 22 Supermicro SuperWorkstation nodes are used for data analysis and visualization jobs. Each node has 36 cores and up to 384 GB memory. 9 of these nodes also feature an NVIDIA Quadro GP100 GPU. 3 nodes feature a single NVIDIA Ampere A100 GPU. 10 nodes feature large-memory, dense GPU configurations to support explorations in machine learning (ML) and deep learning (DL) and general-purpose GPU (GPGPU) computing in atmospheric and related sciences. 4 of these nodes feature 4 NVIDIA Tesla V100 GPUs. 6 of these nodes feature 8 NVIDIA Tesla V100 GPUs. 64 high-throughput computing (HTC) nodes for small computing tasks using 1 or 2 CPUs. 62 HTC nodes have 384 GB of available memory. 2 HTC nodes have 1.5 TB of available memory. 4 nodes are reserved for Research Data Archive workflows. Please refer to the hardware summary table below for detailed specifications. Operating system: CentOS 7.8 Logging in on an NCAR system \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu Or ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. Hardware \u00b6 Data Analysis & Visualization nodes 22 Supermicro 7049GP-TRT SuperWorkstation nodes Up to 384 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR100 InfiniBand link 1 NVIDIA Quadro GP100 GPU 16GB PCIe on each of 9 nodes 1 NVIDIA Ampere A100 GPU 40 GB PCIe on each of 3 nodes Machine Learning/Deep Learning & General Purpose GPU (GPGPU) nodes 4 Supermicro SuperServer nodes with 4 V100 GPUs 768 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters. HDR100 link on each CPU socket 4 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 6 Supermicro SuperServer nodes with 8 V100 GPUs 1152 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters, HDR100 link on each CPU socket 8 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink High-Throughput Computing nodes 62 small-memory workstation nodes 384 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter. HDR100 link on each CPU socket 2 large-memory workstation nodes 1.5 TB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter, HDR100 link on each CPU socket Research Data Archive nodes (reserved for RDA use) 4 Supermicro Workstation nodes 94 GB DDR4-2666 memory per node 2 16-core 2.3-GHz Intel Xeon Gold 5218 (Cascade Lake) processors per node 1.92 TB local Solid State Disk 1 Mellanox ConnectX-6 VPI 100Gb Ethernet connection (GLADE, Campaign Storage, internal connectivity) copy of https://arc.ucar.edu/knowledge_base/70549550","title":"Casper cluster"},{"location":"casper/#casper-cluster","text":"The Casper cluster is a system of specialized data analysis and visualization resources; large-memory, multi-GPU nodes; and high-throughput computing nodes. Casper is composed of 100 nodes featuring Intel Skylake or Cascade Lake processors. 22 Supermicro SuperWorkstation nodes are used for data analysis and visualization jobs. Each node has 36 cores and up to 384 GB memory. 9 of these nodes also feature an NVIDIA Quadro GP100 GPU. 3 nodes feature a single NVIDIA Ampere A100 GPU. 10 nodes feature large-memory, dense GPU configurations to support explorations in machine learning (ML) and deep learning (DL) and general-purpose GPU (GPGPU) computing in atmospheric and related sciences. 4 of these nodes feature 4 NVIDIA Tesla V100 GPUs. 6 of these nodes feature 8 NVIDIA Tesla V100 GPUs. 64 high-throughput computing (HTC) nodes for small computing tasks using 1 or 2 CPUs. 62 HTC nodes have 384 GB of available memory. 2 HTC nodes have 1.5 TB of available memory. 4 nodes are reserved for Research Data Archive workflows. Please refer to the hardware summary table below for detailed specifications. Operating system: CentOS 7.8","title":"Casper cluster"},{"location":"casper/#logging-in-on-an-ncar-system","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu Or ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in.","title":"Logging in on an NCAR system"},{"location":"casper/#hardware","text":"Data Analysis & Visualization nodes 22 Supermicro 7049GP-TRT SuperWorkstation nodes Up to 384 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR100 InfiniBand link 1 NVIDIA Quadro GP100 GPU 16GB PCIe on each of 9 nodes 1 NVIDIA Ampere A100 GPU 40 GB PCIe on each of 3 nodes Machine Learning/Deep Learning & General Purpose GPU (GPGPU) nodes 4 Supermicro SuperServer nodes with 4 V100 GPUs 768 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters. HDR100 link on each CPU socket 4 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 6 Supermicro SuperServer nodes with 8 V100 GPUs 1152 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters, HDR100 link on each CPU socket 8 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink High-Throughput Computing nodes 62 small-memory workstation nodes 384 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter. HDR100 link on each CPU socket 2 large-memory workstation nodes 1.5 TB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter, HDR100 link on each CPU socket Research Data Archive nodes (reserved for RDA use) 4 Supermicro Workstation nodes 94 GB DDR4-2666 memory per node 2 16-core 2.3-GHz Intel Xeon Gold 5218 (Cascade Lake) processors per node 1.92 TB local Solid State Disk 1 Mellanox ConnectX-6 VPI 100Gb Ethernet connection (GLADE, Campaign Storage, internal connectivity) copy of https://arc.ucar.edu/knowledge_base/70549550","title":"Hardware"},{"location":"casper/Compiling%2BGPU%2Bcode%2Bon%2BCasper%2Bnodes/","text":"Compiling GPU code on Casper nodes \u00b6 Below is an example of how to compile and run a GPU code interactively on Casper. You can use the CUDA C source code below to make your own hello_world.cu file. Any libraries you build to support an application should be built with the same compiler, compiler version, and compatible flags that were used to compile the other parts of the application, including the main executable(s). Also, when you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will avoid job failures that can result from missing mpi launchers and library routines. Log in to either Casper or Cheyenne and run execcasper with a GPU resource request to start an interactive job on a GPU-accelerated Casper node. Example: execcasper -l gpu_type=gp100 --ngpus=1 Load the CUDA module when your job starts. module load cuda Use the nvcc command to compile your code. If your code is CUDA Fortran, first load the nvhpc module \u2013 the NVIDIA HPC collection \u2013 and use the compiler command nvfortran instead. nvcc -o hello hello_world.cu Execute the compiled program as usual. ./hello Output: Contents of data before kernel call: HdjikhjcZ Contents of data after kernel call: Hello World! Source code for hello_world.cu \u00b6 / \\ * hello \\ _world . cu ` `\\ * --------------------------------------------------- ` `\\ * A Hello World example in CUDA ` `\\ * --------------------------------------------------- ` `\\ * This is a short program which uses multiple CUDA ` `\\ * threads to calculate a \"Hello World\" message which ` `\\ * is then printed to the screen . It ' s intended to ` `\\ * demonstrate the execution of a CUDA kernel . ` `\\ * --------------------------------------------------- ` `\\ */ #define SIZE 12 #include <stdio.h> #include <stdlib.h> #include <cuda\\_runtime.h> / \\ * CUDA kernel used to calculate hello world message \\ */ \\ _ \\ _global \\ _ \\ _ void hello \\ _world ( char \\ * a , int N ); int main ( int argc , char \\ * \\ * argv ) { ` ` / \\ * data that will live on host \\ */ ` ` char \\ * data ; ` ` / \\ * data that will live in device memory \\ */ ` ` char \\ * d \\ _data ; ` ` / \\ * allocate and initialize data array \\ */ ` ` data = ( char \\ * ) malloc ( SIZE \\ * sizeof ( char )); ` ` data [ 0 ] = 72 ; data [ 1 ] = 100 ; data [ 2 ] = 106 ; ` ` data [ 3 ] = 105 ; data [ 4 ] = 107 ; data [ 5 ] = 27 ; ` ` data [ 6 ] = 81 ; data [ 7 ] = 104 ; data [ 8 ] = 106 ; ` ` data [ 9 ] = 99 ; data [ 10 ] = 90 ; data [ 11 ] = 22 ; ` ` / \\ * print data before kernel call \\ */ ` ` printf ( \"Contents of data before kernel call: %s \\n \" , data ); ` ` / \\ * allocate memory on device \\ */ ` ` cudaMalloc ( & d \\ _data , SIZE \\ * sizeof ( char )); ` ` / \\ * copy memory to device array \\ */ ` ` cudaMemcpy ( d \\ _data , data , SIZE , cudaMemcpyHostToDevice ); ` ` / \\ * call kernel \\ */ ` ` hello \\ _world <<< 4 , 3 >>> ( d \\ _data , SIZE ); ` ` / \\ * copy data back to host \\ */ ` ` cudaMemcpy ( data , d \\ _data , SIZE , cudaMemcpyDeviceToHost ); ` ` / \\ * print contents of array \\ */ ` ` printf ( \"Contents of data after kernel call: %s \\n \" , data ); ` ` / \\ * clean up memory on host and device \\ */ ` ` cudaFree ( d \\ _data ); ` ` free ( data ); ` ` return ( 0 ); } / \\ * hello \\ _world ` `\\ * Each thread increments an element of the input ` `\\ * array by its global thread id ` `\\ */ \\ _ \\ _global \\ _ \\ _ void hello \\ _world ( char \\ * a , int N ) { ` ` int i = blockDim . x \\ * blockIdx . x + threadIdx . x ; ` ` if ( i < N ) a [ i ] = a [ i ] + i ; } Copy of https://arc.ucar.edu/knowledge_base/72581399","title":"Compiling GPU Code on Casper Nodes"},{"location":"casper/Compiling%2BGPU%2Bcode%2Bon%2BCasper%2Bnodes/#compiling-gpu-code-on-casper-nodes","text":"Below is an example of how to compile and run a GPU code interactively on Casper. You can use the CUDA C source code below to make your own hello_world.cu file. Any libraries you build to support an application should be built with the same compiler, compiler version, and compatible flags that were used to compile the other parts of the application, including the main executable(s). Also, when you run the applications, be sure you have loaded the same module/version environment in which you created the applications. This will avoid job failures that can result from missing mpi launchers and library routines. Log in to either Casper or Cheyenne and run execcasper with a GPU resource request to start an interactive job on a GPU-accelerated Casper node. Example: execcasper -l gpu_type=gp100 --ngpus=1 Load the CUDA module when your job starts. module load cuda Use the nvcc command to compile your code. If your code is CUDA Fortran, first load the nvhpc module \u2013 the NVIDIA HPC collection \u2013 and use the compiler command nvfortran instead. nvcc -o hello hello_world.cu Execute the compiled program as usual. ./hello Output: Contents of data before kernel call: HdjikhjcZ Contents of data after kernel call: Hello World!","title":"Compiling GPU code on Casper nodes"},{"location":"casper/Compiling%2BGPU%2Bcode%2Bon%2BCasper%2Bnodes/#source-code-for-hello_worldcu","text":"/ \\ * hello \\ _world . cu ` `\\ * --------------------------------------------------- ` `\\ * A Hello World example in CUDA ` `\\ * --------------------------------------------------- ` `\\ * This is a short program which uses multiple CUDA ` `\\ * threads to calculate a \"Hello World\" message which ` `\\ * is then printed to the screen . It ' s intended to ` `\\ * demonstrate the execution of a CUDA kernel . ` `\\ * --------------------------------------------------- ` `\\ */ #define SIZE 12 #include <stdio.h> #include <stdlib.h> #include <cuda\\_runtime.h> / \\ * CUDA kernel used to calculate hello world message \\ */ \\ _ \\ _global \\ _ \\ _ void hello \\ _world ( char \\ * a , int N ); int main ( int argc , char \\ * \\ * argv ) { ` ` / \\ * data that will live on host \\ */ ` ` char \\ * data ; ` ` / \\ * data that will live in device memory \\ */ ` ` char \\ * d \\ _data ; ` ` / \\ * allocate and initialize data array \\ */ ` ` data = ( char \\ * ) malloc ( SIZE \\ * sizeof ( char )); ` ` data [ 0 ] = 72 ; data [ 1 ] = 100 ; data [ 2 ] = 106 ; ` ` data [ 3 ] = 105 ; data [ 4 ] = 107 ; data [ 5 ] = 27 ; ` ` data [ 6 ] = 81 ; data [ 7 ] = 104 ; data [ 8 ] = 106 ; ` ` data [ 9 ] = 99 ; data [ 10 ] = 90 ; data [ 11 ] = 22 ; ` ` / \\ * print data before kernel call \\ */ ` ` printf ( \"Contents of data before kernel call: %s \\n \" , data ); ` ` / \\ * allocate memory on device \\ */ ` ` cudaMalloc ( & d \\ _data , SIZE \\ * sizeof ( char )); ` ` / \\ * copy memory to device array \\ */ ` ` cudaMemcpy ( d \\ _data , data , SIZE , cudaMemcpyHostToDevice ); ` ` / \\ * call kernel \\ */ ` ` hello \\ _world <<< 4 , 3 >>> ( d \\ _data , SIZE ); ` ` / \\ * copy data back to host \\ */ ` ` cudaMemcpy ( data , d \\ _data , SIZE , cudaMemcpyDeviceToHost ); ` ` / \\ * print contents of array \\ */ ` ` printf ( \"Contents of data after kernel call: %s \\n \" , data ); ` ` / \\ * clean up memory on host and device \\ */ ` ` cudaFree ( d \\ _data ); ` ` free ( data ); ` ` return ( 0 ); } / \\ * hello \\ _world ` `\\ * Each thread increments an element of the input ` `\\ * array by its global thread id ` `\\ */ \\ _ \\ _global \\ _ \\ _ void hello \\ _world ( char \\ * a , int N ) { ` ` int i = blockDim . x \\ * blockIdx . x + threadIdx . x ; ` ` if ( i < N ) a [ i ] = a [ i ] + i ; } Copy of https://arc.ucar.edu/knowledge_base/72581399","title":"Source code for\u00a0hello_world.cu"},{"location":"casper/casper/","text":"Casper cluster \u00b6 The Casper cluster is a system of specialized data analysis and visualization resources; large-memory, multi-GPU nodes; and high-throughput computing nodes. Casper is composed of 100 nodes featuring Intel Skylake or Cascade Lake processors. 22 Supermicro SuperWorkstation nodes are used for data analysis and visualization jobs. Each node has 36 cores and up to 384 GB memory. 9 of these nodes also feature an NVIDIA Quadro GP100 GPU. 3 nodes feature a single NVIDIA Ampere A100 GPU. 10 nodes feature large-memory, dense GPU configurations to support explorations in machine learning (ML) and deep learning (DL) and general-purpose GPU (GPGPU) computing in atmospheric and related sciences. 4 of these nodes feature 4 NVIDIA Tesla V100 GPUs. 6 of these nodes feature 8 NVIDIA Tesla V100 GPUs. 64 high-throughput computing (HTC) nodes for small computing tasks using 1 or 2 CPUs. 62 HTC nodes have 384 GB of available memory. 2 HTC nodes have 1.5 TB of available memory. 4 nodes are reserved for Research Data Archive workflows. Please refer to the hardware summary table below for detailed specifications. Operating system: CentOS 7.8 Logging in on an NCAR system \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu Or ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. Hardware \u00b6 Data Analysis & Visualization nodes 22 Supermicro 7049GP-TRT SuperWorkstation nodes Up to 384 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR100 InfiniBand link 1 NVIDIA Quadro GP100 GPU 16GB PCIe on each of 9 nodes 1 NVIDIA Ampere A100 GPU 40 GB PCIe on each of 3 nodes Machine Learning/Deep Learning & General Purpose GPU (GPGPU) nodes 4 Supermicro SuperServer nodes with 4 V100 GPUs 768 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters. HDR100 link on each CPU socket 4 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 6 Supermicro SuperServer nodes with 8 V100 GPUs 1152 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters, HDR100 link on each CPU socket 8 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink High-Throughput Computing nodes 62 small-memory workstation nodes 384 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter. HDR100 link on each CPU socket 2 large-memory workstation nodes 1.5 TB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter, HDR100 link on each CPU socket Research Data Archive nodes (reserved for RDA use) 4 Supermicro Workstation nodes 94 GB DDR4-2666 memory per node 2 16-core 2.3-GHz Intel Xeon Gold 5218 (Cascade Lake) processors per node 1.92 TB local Solid State Disk 1 Mellanox ConnectX-6 VPI 100Gb Ethernet connection (GLADE, Campaign Storage, internal connectivity) copy of https://arc.ucar.edu/knowledge_base/70549550","title":"Casper cluster"},{"location":"casper/casper/#casper-cluster","text":"The Casper cluster is a system of specialized data analysis and visualization resources; large-memory, multi-GPU nodes; and high-throughput computing nodes. Casper is composed of 100 nodes featuring Intel Skylake or Cascade Lake processors. 22 Supermicro SuperWorkstation nodes are used for data analysis and visualization jobs. Each node has 36 cores and up to 384 GB memory. 9 of these nodes also feature an NVIDIA Quadro GP100 GPU. 3 nodes feature a single NVIDIA Ampere A100 GPU. 10 nodes feature large-memory, dense GPU configurations to support explorations in machine learning (ML) and deep learning (DL) and general-purpose GPU (GPGPU) computing in atmospheric and related sciences. 4 of these nodes feature 4 NVIDIA Tesla V100 GPUs. 6 of these nodes feature 8 NVIDIA Tesla V100 GPUs. 64 high-throughput computing (HTC) nodes for small computing tasks using 1 or 2 CPUs. 62 HTC nodes have 384 GB of available memory. 2 HTC nodes have 1.5 TB of available memory. 4 nodes are reserved for Research Data Archive workflows. Please refer to the hardware summary table below for detailed specifications. Operating system: CentOS 7.8","title":"Casper cluster"},{"location":"casper/casper/#logging-in-on-an-ncar-system","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu Or ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling SSH to enable X11 forwarding. You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in.","title":"Logging in on an NCAR system"},{"location":"casper/casper/#hardware","text":"Data Analysis & Visualization nodes 22 Supermicro 7049GP-TRT SuperWorkstation nodes Up to 384 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR100 InfiniBand link 1 NVIDIA Quadro GP100 GPU 16GB PCIe on each of 9 nodes 1 NVIDIA Ampere A100 GPU 40 GB PCIe on each of 3 nodes Machine Learning/Deep Learning & General Purpose GPU (GPGPU) nodes 4 Supermicro SuperServer nodes with 4 V100 GPUs 768 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters. HDR100 link on each CPU socket 4 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink 6 Supermicro SuperServer nodes with 8 V100 GPUs 1152 GB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6140 (Skylake) processors per node 2 TB local NVMe Solid State Disk 1 Mellanox ConnectX-4 100Gb Ethernet connection (GLADE, Campaign Storage, external connectivity) 2 Mellanox ConnectX-6 HDR200 InfiniBand adapters, HDR100 link on each CPU socket 8 NVIDIA Tesla V100 32GB SXM2 GPUs with NVLink High-Throughput Computing nodes 62 small-memory workstation nodes 384 GB DDR4-2666 memory per node 2 18-core 2.6-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter. HDR100 link on each CPU socket 2 large-memory workstation nodes 1.5 TB DDR4-2666 memory per node 2 18-core 2.3-GHz Intel Xeon Gold 6240 (Cascade Lake) processors per node 1.6 TB local NVMe Solid State Disk 1 Mellanox ConnectX-5 100Gb Ethernet VPI adapter (GLADE, Campaign Storage, external connectivity) 1 Mellanox ConnectX-6 HDR200 InfiniBand VPI adapter, HDR100 link on each CPU socket Research Data Archive nodes (reserved for RDA use) 4 Supermicro Workstation nodes 94 GB DDR4-2666 memory per node 2 16-core 2.3-GHz Intel Xeon Gold 5218 (Cascade Lake) processors per node 1.92 TB local Solid State Disk 1 Mellanox ConnectX-6 VPI 100Gb Ethernet connection (GLADE, Campaign Storage, internal connectivity) copy of https://arc.ucar.edu/knowledge_base/70549550","title":"Hardware"},{"location":"cheyenne/","text":"Cheyenne supercomputer \u00b6 Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne Logging in \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. Hardware \u00b6 145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504) Estimating core-hours needed \u00b6 Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Cheyenne supercomputer"},{"location":"cheyenne/#cheyenne-supercomputer","text":"Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne","title":"Cheyenne supercomputer"},{"location":"cheyenne/#logging-in","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in.","title":"Logging in"},{"location":"cheyenne/#hardware","text":"145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504)","title":"Hardware"},{"location":"cheyenne/#estimating-core-hours-needed","text":"Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Estimating core-hours needed"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/","text":"Cheyenne use policies \u00b6 Page contents \u00b6 Appropriate use of login nodes Fair share policy Job scheduling priorities Appropriate use of login nodes \u00b6 Users may run short, non-memory-intensive processes interactively on the Cheyenne system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. Warning The login nodes should not be used to run processes that consume excessive resources. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster. See Checking memory use for how to use the peak_memusage utility. Fair share policy \u00b6 CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, and the Wyoming community. The fair-share policy takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions. Job scheduling priorities \u00b6 The PBS Pro workload management system scheduling policy for running jobs in the Cheyenne environment requires balancing several factors, as summarized above. Jobs generally are sorted in the following order: Current processor use Job priority Queue priority Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: Your job is waiting for resources. The system has been reserved for a scheduled outage. Your job has been placed on hold. You have reached your concurrent core-usage limit when using the share queue . Note that a high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command only sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues for an indefinite time, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job. PBS sorting order \u00b6 Stakeholder shares \u00b6 CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the Climate Simulation Laboratory, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In this case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares. Job priority \u00b6 Job priority has three components. native priority queue priority waiting time If the native priority is 0, a further adjustment is made based on how long the job has been waiting for resources. Waiting jobs get a \"boost\" of up to 20 priority points, depending on how long they have been waiting and which queue they are in. Queue priority \u00b6 Cheyenne queue priorities are shown in the table on this web page: Job-submission queues and charges . Job size \u00b6 Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs to start. Backfilling \u00b6 As mentioned above, when PBS cannot start a job immediately, if it is one of the first such jobs, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.","title":"Cheyenne Use Policies"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#cheyenne-use-policies","text":"","title":"Cheyenne use policies"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#page-contents","text":"Appropriate use of login nodes Fair share policy Job scheduling priorities","title":"Page contents"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#appropriate-use-of-login-nodes","text":"Users may run short, non-memory-intensive processes interactively on the Cheyenne system's login nodes. These include tasks such as text editing or running small serial scripts or programs. However, the login nodes may not be used to run processes that consume excessive resources. This is to ensure an appropriate balance between user convenience and login node performance. Warning The login nodes should not be used to run processes that consume excessive resources. This applies to individual processes that consume excessive amounts of CPU time, more than a few GB of memory, or excessive I/O resources. It also applies collectively to multiple concurrent tasks that an individual user runs. Processes that use excessive resources on the login nodes are terminated automatically. Affected users are informed by email that their sessions were terminated. They are also advised to run such processes in batch or interactive jobs on the Casper cluster. See Checking memory use for how to use the peak_memusage utility.","title":"Appropriate use of login nodes"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#fair-share-policy","text":"CISL manages scheduling priorities to ensure fair access to the system by all of these stakeholder groups: the university community, the NCAR community, the Community Earth System Model (CESM) community, and the Wyoming community. The fair-share policy takes the community-wide usage balance into account along with several additional factors. These include the submitting users' currently running jobs and recently completed jobs. The scheduling system uses a dynamic-priority formula to weigh these factors, calculate each job's priority, and make scheduling decisions.","title":"Fair share policy"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#job-scheduling-priorities","text":"The PBS Pro workload management system scheduling policy for running jobs in the Cheyenne environment requires balancing several factors, as summarized above. Jobs generally are sorted in the following order: Current processor use Job priority Queue priority Job size Job sorting is adjusted frequently in response to varying demands and workloads. PBS examines the jobs in sorted order in each scheduling cycle and starts those that it can. Jobs that cannot be started immediately are either scheduled to run at a future time or bypassed for the current cycle. Under typical system usage, multiple scheduling cycles are initiated every minute. The scheduler may not start a job for a number of reasons, including: Your job is waiting for resources. The system has been reserved for a scheduled outage. Your job has been placed on hold. You have reached your concurrent core-usage limit when using the share queue . Note that a high-priority job might be delayed by one of the limits on the list, while a lower-priority job from a different user or a job requesting fewer resources might not be blocked. If your job is waiting in the queue, you can run the qstat command as shown to obtain information that can indicate why it has not started running. (Use this command only sparingly.) qstat -s jobID Note To prevent jobs from languishing in the queues for an indefinite time, PBS reserves resources for the top-priority jobs and doesn't allow lower-priority jobs to start if they would delay the start time of a higher-priority job.","title":"Job scheduling priorities"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#pbs-sorting-order","text":"","title":"PBS sorting order"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#stakeholder-shares","text":"CISL manages scheduling priorities to ensure fair access to the system by these stakeholder groups: the university community, the NCAR community, the Climate Simulation Laboratory, and the Wyoming community. Each stakeholder group is allocated a certain percentage of the available processors. A job cannot start if that action would cause the group to exceed its share, unless another group is using less than its share and has no jobs waiting. In this case, the high-use group can \"borrow\" processors from the lower-use stakeholder group for a short time. When jobs are sorted, jobs from groups that are using less of their share are picked before jobs from groups using more of their shares.","title":"Stakeholder shares"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#job-priority","text":"Job priority has three components. native priority queue priority waiting time If the native priority is 0, a further adjustment is made based on how long the job has been waiting for resources. Waiting jobs get a \"boost\" of up to 20 priority points, depending on how long they have been waiting and which queue they are in.","title":"Job priority"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#queue-priority","text":"Cheyenne queue priorities are shown in the table on this web page: Job-submission queues and charges .","title":"Queue priority"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#job-size","text":"Jobs asking for more nodes are favored over jobs asking for fewer. The reasoning is that while it is easier for small jobs to fill gaps in the schedule, larger jobs need help collecting enough CPUs to start.","title":"Job size"},{"location":"cheyenne/Cheyenne%2Buse%2Bpolicies/#backfilling","text":"As mentioned above, when PBS cannot start a job immediately, if it is one of the first such jobs, PBS sets aside resources for it before examining other jobs to see if any of them can run as backfill. That is, PBS looks at running jobs to determine when they will finish based on wall-time requested. From those finish times, PBS decides when enough resources (such as CPUs, memory, and job limits) will become available to run the top job. PBS then reserves the resources that the job requests at that identified time. When PBS looks at other jobs to see if they can start immediately, it also checks whether starting any of them would collide with one of these resource reservations. Only if there are no collisions will PBS start the lower-priority jobs.","title":"Backfilling"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/","text":"Quick start on Cheyenne \u00b6 Once you have an account and the necessary software , you can log in and run jobs on the Cheyenne supercomputer. Logging in also gives users access to these resources, depending on their allocations: GLADE centralized file service Campaign Storage file system To run data analysis and visualization jobs on the Casper system's nodes, follow the procedures described here . There is no need to transfer output files from Cheyenne for this since Cheyenne and Casper mount the same GLADE file systems. Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators. Page contents \u00b6 Logging in on an NCAR system Environment Compiling Debugging Cheyenne queues Submitting jobs Logging in on an NCAR system \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@cheyenne.ucar.edu OR ssh -X username@casper.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X cheyenne.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. Environment \u00b6 The Cheyenne HPC system uses a Linux operating system and supports widely used shells on its login and compute nodes. Users also have several compiler choices. Operating system \u00b6 SUSE Linux. Scheduler \u00b6 Altair PBS Pro. See this detailed documentation about using PBS Pro and running jobs. Shells \u00b6 The default login shell for new Cheyenne users is bash . You can change the default after logging in to the Systems Accounting Manager ( SAM ). It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering echo $SHELL on your Cheyenne command line. Environment modules \u00b6 The Cheyenne module utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. Here are some of the most commonly used commands. (See the Environment modules page for more details.) module av - Show which modules are available for use with the current compiler. module help - List switches, subcommands, and arguments for the module utility. Specify a modulefile by name for help with an individual module. module help netcdf module list - List the modules that are loaded. module load - Load the default version of a software package, or load a specified version. module load modulefile\\_name module load modulefile\\_name/n.n.n module spider - List all modules that exist on the system. module swap - Unload one module and load a different one. Example: module swap netcdf pnetcdf module unload - Unload the specified software package. module unload modulefile\\_name Compiling \u00b6 Cheyenne users have access to Intel, PGI, and GNU compilers. The Intel compiler module is loaded by default. After loading the compiler module that you want to use, identify and run the appropriate compilation wrapper command from the table below. (If your script already includes one of the following generic MPI commands, there is no need to change it: mpif90 , mpif77 , ftn ; mpicc , cc ; mpiCC and CC .) Also consider using the compiler's diagnostic flags to identify potential problems. Compiler Language Commands for serial programs Commands for programs using MPI Flags to enable OpenMP (for serial and MPI) Intel (default) Fortran ifort foo.f90 mpif90 foo.f90 -qopenmp C icc foo.c mpicc foo.c -qopenmp C++ icpc foo.C mpicxx foo.C -qopenmp Include these flags for best performance when you use the Intel compiler: -march=corei7 -axCORE-AVX2 NVIDIA HPC Fortran nvfortran foo.f90 mpif90 foo.f90 -mp C nvc foo.c mpicc foo.c -mp C++ nvc++ foo.C mpicxx foo.C -mp GNU (GCC) Fortran gfortran foo.f90 mpif90 foo.f90 -fopenmp C gcc foo.c mpicc foo.c -fopenmp C++ g++ foo.C mpicxx foo.C -fopenmp PGI Fortran pgfortran (or pgf90 or pgf95) foo.f90 mpif90 foo.f90 -mp C pgcc foo.c mpicc foo.c -mp C++ pgcpp (or pgCC) foo.C mpicxx foo.C -mp The PGI compiler has become the NVIDIA HPC (nvhpc) compiler and all future versions will be released as such. PGI users should migrate to NVIDIA when possible. Debugging \u00b6 CISL provides the ARM Forge tools, DDT and MAP , for debugging, profiling, and optimizing code in the Cheyenne environment. Performance Reports is another ARM tool for Cheyenne users. It summarizes the performance of HPC application runs. See Running DDT, MAP and PR jobs . Cheyenne queues \u00b6 Most of the Cheyenne batch queues are for exclusive use, and jobs are charged for all 36 cores on each node that is used. Jobs in the shared use share queue are charged only for the cores used. The regular queue, which has a 12-hour wall-clock limit, meets most users' needs for running batch jobs. See the table on this page for information about other queues. Submitting jobs \u00b6 Schedule your jobs to run on Cheyenne by submitting them through the PBS Pro workload management system. (To run jobs on the Casper cluster, see this documentation .) To submit a Cheyenne batch job, use the qsub command followed by the name of your batch script file. qsub script_name import khar as kh See this page for job script examples. To start an interactive job, use the qsub command with the necessary options but no script file. qsub -I -l select=1:ncpus=36:mpiprocs=36 -l walltime=01:00:00 -q regular -A project_code More detailed PBS Pro documentation \u00b6 Submitting jobs with PBS Scripts that you can adapt for your own jobs copy of https://arc.ucar.edu/knowledge_base/72581213","title":"Quick Start on Cheyenne"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#quick-start-on-cheyenne","text":"Once you have an account and the necessary software , you can log in and run jobs on the Cheyenne supercomputer. Logging in also gives users access to these resources, depending on their allocations: GLADE centralized file service Campaign Storage file system To run data analysis and visualization jobs on the Casper system's nodes, follow the procedures described here . There is no need to transfer output files from Cheyenne for this since Cheyenne and Casper mount the same GLADE file systems. Don\u2019t run sudo on NCAR systems! If you need help with tasks that you think require sudo privileges, or if you aren\u2019t sure, please contact HPC User Support before trying to run sudo yourself. The command fails when unauthorized users run it and sends a security alert to system administrators.","title":"Quick start on Cheyenne"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#page-contents","text":"Logging in on an NCAR system Environment Compiling Debugging Cheyenne queues Submitting jobs","title":"Page contents"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#logging-in-on-an-ncar-system","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@cheyenne.ucar.edu OR ssh -X username@casper.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X cheyenne.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in.","title":"Logging in on an NCAR system"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#environment","text":"The Cheyenne HPC system uses a Linux operating system and supports widely used shells on its login and compute nodes. Users also have several compiler choices.","title":"Environment"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#operating-system","text":"SUSE Linux.","title":"Operating system"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#scheduler","text":"Altair PBS Pro. See this detailed documentation about using PBS Pro and running jobs.","title":"Scheduler"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#shells","text":"The default login shell for new Cheyenne users is bash . You can change the default after logging in to the Systems Accounting Manager ( SAM ). It may take several hours for a change you make to take effect. You can confirm which shell is set as your default by entering echo $SHELL on your Cheyenne command line.","title":"Shells"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#environment-modules","text":"The Cheyenne module utility enables users to easily load and unload compilers and compatible software packages as needed, and to create multiple customized environments for various tasks. Here are some of the most commonly used commands. (See the Environment modules page for more details.) module av - Show which modules are available for use with the current compiler. module help - List switches, subcommands, and arguments for the module utility. Specify a modulefile by name for help with an individual module. module help netcdf module list - List the modules that are loaded. module load - Load the default version of a software package, or load a specified version. module load modulefile\\_name module load modulefile\\_name/n.n.n module spider - List all modules that exist on the system. module swap - Unload one module and load a different one. Example: module swap netcdf pnetcdf module unload - Unload the specified software package. module unload modulefile\\_name","title":"Environment modules"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#compiling","text":"Cheyenne users have access to Intel, PGI, and GNU compilers. The Intel compiler module is loaded by default. After loading the compiler module that you want to use, identify and run the appropriate compilation wrapper command from the table below. (If your script already includes one of the following generic MPI commands, there is no need to change it: mpif90 , mpif77 , ftn ; mpicc , cc ; mpiCC and CC .) Also consider using the compiler's diagnostic flags to identify potential problems.","title":"Compiling"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#debugging","text":"CISL provides the ARM Forge tools, DDT and MAP , for debugging, profiling, and optimizing code in the Cheyenne environment. Performance Reports is another ARM tool for Cheyenne users. It summarizes the performance of HPC application runs. See Running DDT, MAP and PR jobs .","title":"Debugging"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#cheyenne-queues","text":"Most of the Cheyenne batch queues are for exclusive use, and jobs are charged for all 36 cores on each node that is used. Jobs in the shared use share queue are charged only for the cores used. The regular queue, which has a 12-hour wall-clock limit, meets most users' needs for running batch jobs. See the table on this page for information about other queues.","title":"Cheyenne queues"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#submitting-jobs","text":"Schedule your jobs to run on Cheyenne by submitting them through the PBS Pro workload management system. (To run jobs on the Casper cluster, see this documentation .) To submit a Cheyenne batch job, use the qsub command followed by the name of your batch script file. qsub script_name import khar as kh See this page for job script examples. To start an interactive job, use the qsub command with the necessary options but no script file. qsub -I -l select=1:ncpus=36:mpiprocs=36 -l walltime=01:00:00 -q regular -A project_code","title":"Submitting jobs"},{"location":"cheyenne/Quick%2Bstart%2Bon%2BCheyenne/#more-detailed-pbs-pro-documentation","text":"Submitting jobs with PBS Scripts that you can adapt for your own jobs copy of https://arc.ucar.edu/knowledge_base/72581213","title":"More detailed PBS Pro documentation"},{"location":"cheyenne/cheyenne/","text":"Cheyenne supercomputer \u00b6 Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne Logging in \u00b6 To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in. Hardware \u00b6 145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504) Estimating core-hours needed \u00b6 Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Cheyenne supercomputer"},{"location":"cheyenne/cheyenne/#cheyenne-supercomputer","text":"Cheyenne is a 5.34-petaflops, high-performance computer built for NCAR by SGI. The system was released for production work on January 12, 2017. An SGI ICE XA Cluster, the Cheyenne supercomputer features 145,152 Intel Xeon processor cores in 4,032 dual-socket nodes (36 cores/node) and 313 TB of total memory. Cheyenne's login nodes give users access to the GLADE shared-disk resource and other storage systems. Data storage components provided by DataDirect Networks (DDN) give the GLADE system a total usable capacity of 38 PB. The DDN system transfers data at the rate of 200 GBps, more than twice as fast as the previous file system\u2019s rate of 90 GBps. Go to Quick start on Cheyenne","title":"Cheyenne supercomputer"},{"location":"cheyenne/cheyenne/#logging-in","text":"To log in, start your terminal or Secure Shell client and run an ssh command as shown here: ssh -X username@system_name.ucar.edu OR ssh -X username@system_name.ucar.edu Some users (particularly on Macs) need to use -Y instead of -X when calling ssh to enable X11 forwarding . You can use this shorter command if your username for the system is the same as your username on your local computer: ssh -X system_name.ucar.edu OR ssh -X system_name.ucar.edu After running the ssh command, you will be asked to authenticate to finish logging in.","title":"Logging in"},{"location":"cheyenne/cheyenne/#hardware","text":"145,152 processor cores 2.3-GHz Intel Xeon E5-2697V4 (Broadwell) processors 16 flops per clock 4,032 computation nodes Dual-socket nodes, 18 cores per socket 6 login nodes Dual-socket nodes, 18 cores per socket 256 GB memory/node 313 TB total system memory 64 GB/node on 3,168 nodes, DDR4-2400 128 GB/node on 864 nodes, DDR4-2400 Mellanox EDR InfiniBand high-speed interconnect Partial 9D Enhanced Hypercube single-plane interconnect topology Bandwidth: 25 GBps bidirectional per link Latency: MPI ping-pong < 1 \u00b5s; hardware link 130 ns 3 times Yellowstone computational capacity Comparison based on the relative performance of CISL High Performance Computing Benchmarks run on each system. > 3.5 times Yellowstone peak performance 5.34 peak petaflops (vs. 1.504)","title":"Hardware"},{"location":"cheyenne/cheyenne/#estimating-core-hours-needed","text":"Cheyenne allocations are made in core-hours. The recommended method for estimating your resource needs for an allocation request is to perform benchmark runs. Some guidance is provided here. The core-hours used for a job are calculated by multiplying the number of processor cores used by the wall-clock duration in hours. Cheyenne core-hour calculations should assume that all jobs will run in the regular queue and that they are charged for use of all 36 cores on each node. Just a copy of https://arc.ucar.edu/knowledge_base/70549542","title":"Estimating core-hours needed"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/","text":"Job-submission queues and charges \u00b6 Most Cheyenne batch queues are for exclusive use, so jobs submitted to those queues are charged for all 36 cores on each node that is used. Jobs in the shared use share queue are charged only for the cores that are used. The regular queue, which has a 12-hour wall-clock limit, meets most users' needs for running batch jobs on Cheyenne. Page contents \u00b6 Cheyenne queue details Calculating charges Exclusive nodes Shared nodes (Cheyenne and Casper) Checking and managing charges Cheyenne queue details \u00b6 Queue name Priority order Wall clock limit (hours) Queue factor Cheyenne queue description premium 1 12 1.5 Jobs are charged at 150% of the regular rate. regular 2 12 1 Most production batch jobs run in this queue; also accepts interactive jobs. economy 3 12 0.7 Production batch jobs are charged at 70% of the regular rate. share NA 6 1 Interactive and serial batch use for debugging and other tasks on a single, shared, 128-GB node. An individual job can use up to 18 cores. A user can run multiple jobs in the share queue concurrently if the total number of cores they require is no more than 18. Additional jobs that the user submits remain in the queue to run later. Some additional queues on the system are for dedicated purposes and accessible only to authorized users. Casper: See this page for information on running jobs on Casper nodes. Calculating charges \u00b6 Exclusive nodes \u00b6 Charges for use of Cheyenne are calculated in terms of core-hours. Jobs run in Cheyenne queues other than \"share\" are charged for exclusive use of the nodes by this formula: wall-clock hours \u00d7 nodes used \u00d7 cores per node \u00d7 queue factor Number of nodes used \u00b6 Your batch script indicates how many Cheyenne nodes your job will use. In this example, you have selected 2 nodes, each of which has 36 cores. Your job will be charged for the use of 72 cores. #PBS -l select=2:ncpus=36:mpiprocs=36 Shared nodes (Cheyenne and Casper) \u00b6 Charges for jobs that you run on a shared node, including Casper nodes, are calculated by this formula: core-seconds/3600 (core-hours) Checking and managing charges \u00b6 Users can check computing and storage charges through the CISL Systems Accounting Manager. (Go to documentation or to SAM app .) If you have concerns about using your allocation most efficiently, contact the NCAR Research Computing help desk for guidance. Sometimes jobs can be configured to make better use of the processors, and you may be able to save by using a less expensive queue. CISL can refund core-hours if system failures cause jobs to fail and the failed jobs are reported promptly. Use this core-hours refund request form (login required) if you think a refund is warranted. Technical limitations prevent us from verifying refund eligibility for jobs that are more than seven days old. This page is just a copy of https://arc.ucar.edu/knowledge_base/72581495 in mkdown for demonstrations.","title":"Job-submission Queues and Charges"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#job-submission-queues-and-charges","text":"Most Cheyenne batch queues are for exclusive use, so jobs submitted to those queues are charged for all 36 cores on each node that is used. Jobs in the shared use share queue are charged only for the cores that are used. The regular queue, which has a 12-hour wall-clock limit, meets most users' needs for running batch jobs on Cheyenne.","title":"Job-submission queues and charges"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#page-contents","text":"Cheyenne queue details Calculating charges Exclusive nodes Shared nodes (Cheyenne and Casper) Checking and managing charges","title":"Page contents"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#cheyenne-queue-details","text":"Queue name Priority order Wall clock limit (hours) Queue factor Cheyenne queue description premium 1 12 1.5 Jobs are charged at 150% of the regular rate. regular 2 12 1 Most production batch jobs run in this queue; also accepts interactive jobs. economy 3 12 0.7 Production batch jobs are charged at 70% of the regular rate. share NA 6 1 Interactive and serial batch use for debugging and other tasks on a single, shared, 128-GB node. An individual job can use up to 18 cores. A user can run multiple jobs in the share queue concurrently if the total number of cores they require is no more than 18. Additional jobs that the user submits remain in the queue to run later. Some additional queues on the system are for dedicated purposes and accessible only to authorized users. Casper: See this page for information on running jobs on Casper nodes.","title":"Cheyenne queue details"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#calculating-charges","text":"","title":"Calculating charges"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#exclusive-nodes","text":"Charges for use of Cheyenne are calculated in terms of core-hours. Jobs run in Cheyenne queues other than \"share\" are charged for exclusive use of the nodes by this formula: wall-clock hours \u00d7 nodes used \u00d7 cores per node \u00d7 queue factor","title":"Exclusive nodes"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#number-of-nodes-used","text":"Your batch script indicates how many Cheyenne nodes your job will use. In this example, you have selected 2 nodes, each of which has 36 cores. Your job will be charged for the use of 72 cores. #PBS -l select=2:ncpus=36:mpiprocs=36","title":"Number of nodes used"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#shared-nodes-cheyenne-and-casper","text":"Charges for jobs that you run on a shared node, including Casper nodes, are calculated by this formula: core-seconds/3600 (core-hours)","title":"Shared nodes (Cheyenne and Casper)"},{"location":"cheyenne/starting-cheyenne-jobs/Job-submission%2Bqueues%2Band%2Bcharges/#checking-and-managing-charges","text":"Users can check computing and storage charges through the CISL Systems Accounting Manager. (Go to documentation or to SAM app .) If you have concerns about using your allocation most efficiently, contact the NCAR Research Computing help desk for guidance. Sometimes jobs can be configured to make better use of the processors, and you may be able to save by using a less expensive queue. CISL can refund core-hours if system failures cause jobs to fail and the failed jobs are reported promptly. Use this core-hours refund request form (login required) if you think a refund is warranted. Technical limitations prevent us from verifying refund eligibility for jobs that are more than seven days old. This page is just a copy of https://arc.ucar.edu/knowledge_base/72581495 in mkdown for demonstrations.","title":"Checking and managing charges"},{"location":"cheyenne/starting-cheyenne-jobs/Managing%2Band%2Bmonitoring%2BPBS%2Bjobs/","text":"Managing and monitoring PBS jobs \u00b6 Here are some of the most useful commands for managing and monitoring jobs launched with PBS and running on Casper or Cheyenne nodes. Most of these commands will only modify or query data from jobs that are active on the same system. That is, run each command on Cheyenne if you want to interact with a job on Cheyenne. Tip Run any of these commands followed by -h to get help, as in qhist -h . Page contents \u00b6 qdel qhist qstat qdel \u00b6 Run qdel with the job ID to kill a pending or running job. qdel jobID Kill all of your own pending or running jobs. (Be sure to use backticks as shown.) qdel `qselect -u $USER` qhist \u00b6 Run qhist for information on finished jobs. qhist -u $USER Your output will include jobs that finished on the current day unless you specify the number ( N ) of days to include. qhist -u $USER -d N Your output will be similar to this, with Mem(GB) and CPU(%) indicating approximate total memory usage per job and average CPU usage per core per job : Job ID User Queue Nodes NCPUs Finish RMem(GB) Mem(GB) CPU(%) Elap(h) 2426690 stormyk regular 1 1 05-1527 - 0.3 75.0 0.09 2426693 stormyk regular 1 1 05-1527 - 0.1 90.0 0.09 2426541 stormyk regular 1 1 05-1523 - 0.1 83.0 0.03 2426542 stormyk regular 1 1 05-1524 - 0.1 70.0 0.04 2426683 stormyk regular 1 1 05-1523 - 0.1 0.0 0.02 2426444 stormyk regular 1 1 05-1522 - 0.1 19.0 0.02 2426435 stormyk regular 1 1 05-1522 - 0.1 13.0 0.02 The following variation will generate a list of jobs that finished with non-zero exit codes to help you identify jobs that failed. qhist -u $USER -r x0 qstat \u00b6 Run this to see the status of all of your own unfinished jobs. qstat -u $USER Your output will be similar to what is shown just below. Most column headings are self-explanatory \u2013 NDS for nodes, TSK for tasks, and so on. In the status (S) column, most jobs are either queued (Q) or running (R) . Sometimes jobs are held (H) , which might mean they are dependent on the completion of another job. If you have a job that is held and is not dependent on another job, CISL recommends killing and resubmitting the job. `Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time \\------ -------- ----- ------- ------ --- --- ------ ----- - ---- 657237\\.chadmin apatelsm economy ens603 46100 60 216 -- 02:30 R 01:24 657238\\.chadmin apatelsm regular ens605 -- 1 36 -- 00:05 H -- 657466\\.chadmin apatelsm economy ens701 5189 60 216 -- 02:30 R 00:46 657467\\.chadmin apatelsm regular ens703 -- 1 36 -- 00:10 H -- Following are examples of qstat with some other commonly used options and arguments. Get a long-form summary of the status of an unfinished job. (Use this only sparingly; it places a high load on PBS.) qstat -f jobID Get a single-line summary of the status of an unfinished or recently completed job (within 72 hours). qstat -x jobID Get information about unfinished jobs in a specified queue. qstat queue\\_name See job activity by queue (e.g., pending, running) in terms of numbers of jobs. qstat -Q Display information for all of your pending, running, and finished jobs. qstat -x -u $USER Tip Query jobs running on one system by specifying @cheyenne or @casper from either system as shown here. (Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s ). qstat -w -u $USER @cheyenne This page is just a copy of https://arc.ucar.edu/knowledge_base/68878389 in mkdown for demonstrations.","title":"Managing and monitoring PBS jobs"},{"location":"cheyenne/starting-cheyenne-jobs/Managing%2Band%2Bmonitoring%2BPBS%2Bjobs/#managing-and-monitoring-pbs-jobs","text":"Here are some of the most useful commands for managing and monitoring jobs launched with PBS and running on Casper or Cheyenne nodes. Most of these commands will only modify or query data from jobs that are active on the same system. That is, run each command on Cheyenne if you want to interact with a job on Cheyenne. Tip Run any of these commands followed by -h to get help, as in qhist -h .","title":"Managing and monitoring PBS jobs"},{"location":"cheyenne/starting-cheyenne-jobs/Managing%2Band%2Bmonitoring%2BPBS%2Bjobs/#page-contents","text":"qdel qhist qstat","title":"Page contents"},{"location":"cheyenne/starting-cheyenne-jobs/Managing%2Band%2Bmonitoring%2BPBS%2Bjobs/#qdel","text":"Run qdel with the job ID to kill a pending or running job. qdel jobID Kill all of your own pending or running jobs. (Be sure to use backticks as shown.) qdel `qselect -u $USER`","title":"qdel"},{"location":"cheyenne/starting-cheyenne-jobs/Managing%2Band%2Bmonitoring%2BPBS%2Bjobs/#qhist","text":"Run qhist for information on finished jobs. qhist -u $USER Your output will include jobs that finished on the current day unless you specify the number ( N ) of days to include. qhist -u $USER -d N Your output will be similar to this, with Mem(GB) and CPU(%) indicating approximate total memory usage per job and average CPU usage per core per job : Job ID User Queue Nodes NCPUs Finish RMem(GB) Mem(GB) CPU(%) Elap(h) 2426690 stormyk regular 1 1 05-1527 - 0.3 75.0 0.09 2426693 stormyk regular 1 1 05-1527 - 0.1 90.0 0.09 2426541 stormyk regular 1 1 05-1523 - 0.1 83.0 0.03 2426542 stormyk regular 1 1 05-1524 - 0.1 70.0 0.04 2426683 stormyk regular 1 1 05-1523 - 0.1 0.0 0.02 2426444 stormyk regular 1 1 05-1522 - 0.1 19.0 0.02 2426435 stormyk regular 1 1 05-1522 - 0.1 13.0 0.02 The following variation will generate a list of jobs that finished with non-zero exit codes to help you identify jobs that failed. qhist -u $USER -r x0","title":"qhist"},{"location":"cheyenne/starting-cheyenne-jobs/Managing%2Band%2Bmonitoring%2BPBS%2Bjobs/#qstat","text":"Run this to see the status of all of your own unfinished jobs. qstat -u $USER Your output will be similar to what is shown just below. Most column headings are self-explanatory \u2013 NDS for nodes, TSK for tasks, and so on. In the status (S) column, most jobs are either queued (Q) or running (R) . Sometimes jobs are held (H) , which might mean they are dependent on the completion of another job. If you have a job that is held and is not dependent on another job, CISL recommends killing and resubmitting the job. `Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time \\------ -------- ----- ------- ------ --- --- ------ ----- - ---- 657237\\.chadmin apatelsm economy ens603 46100 60 216 -- 02:30 R 01:24 657238\\.chadmin apatelsm regular ens605 -- 1 36 -- 00:05 H -- 657466\\.chadmin apatelsm economy ens701 5189 60 216 -- 02:30 R 00:46 657467\\.chadmin apatelsm regular ens703 -- 1 36 -- 00:10 H -- Following are examples of qstat with some other commonly used options and arguments. Get a long-form summary of the status of an unfinished job. (Use this only sparingly; it places a high load on PBS.) qstat -f jobID Get a single-line summary of the status of an unfinished or recently completed job (within 72 hours). qstat -x jobID Get information about unfinished jobs in a specified queue. qstat queue\\_name See job activity by queue (e.g., pending, running) in terms of numbers of jobs. qstat -Q Display information for all of your pending, running, and finished jobs. qstat -x -u $USER Tip Query jobs running on one system by specifying @cheyenne or @casper from either system as shown here. (Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s ). qstat -w -u $USER @cheyenne This page is just a copy of https://arc.ucar.edu/knowledge_base/68878389 in mkdown for demonstrations.","title":"qstat"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/","text":"Cheyenne job script examples \u00b6 When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting TMPDIR in your batch scripts as recommended here: Storing temporary files with TMPDIR . Specify an ompthreads value in your script to help ensure that any parallel job will run properly. This is particularly important in a couple of cases: If your code was compiled with the GNU or Intel compiler using the -f openmp option. If your code was compiled with a PGI compiler with option -mp . Tip For a parallel job that does not use OpenMP threads \u2013 for a pure MPI job, for example \u2013 specify ompthreads=1 in your PBS select statement as shown below. Failure to do so may result in the job oversubscribing its nodes, resulting in poor performance or puzzling behavior such as exceeding its wallclock limit. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel mpt If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/19.1.1 mpt/2.25 These examples are based on the following assumptions: You are using HPE's Message Passing Toolkit (MPT) MPI library. The programs being run were compiled with Intel 16.0.3 or a later version. Contact the NCAR Research Computing help desk for assistance with adapting them for other cases. When your script is ready, submit your batch job for scheduling as shown here . Page contents \u00b6 Batch script to run an MPI job Batch script to run a pure OpenMP job Batch script to run a hybrid MPI/OpenMP job Batch script to run a job array Batch script to run a command file (MPMD) job Pinning tasks/threads to CPUs Dependent jobs Batch script to run an MPI job \u00b6 tcsh bash #!/bin/tcsh ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe #!/bin/bash ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe Batch script to run a pure OpenMP job \u00b6 To run a pure OpenMP job, specify the number of CPUs you want from the node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name #!/bin/bash #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name Batch script to run a hybrid MPI/OpenMP job \u00b6 If you want to run a hybrid MPI/OpenMP configuration where each node uses threaded parallelism while the nodes communicate with each other using MPI, activate NUMA mode and run using the MPI launcher. Specify the number of CPUs you want from each node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name Batch script to run a job array \u00b6 Job arrays are useful when you want to run the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. The elements in a job array are known as \"sub-jobs.\" Before submitting this batch script: Place files input.1 through input.18 in the same directory where you have the sequential cmd command. The batch job specifies 18 sub-jobs indexed 1-18 that will run in the \"share\" queue. The Nth sub-job uses file input.N to produce file output.N. The \"share\" queue is recommended for running job arrays of sequential sub-jobs, or parallel sub-jobs each having from two to nine tasks. The share queue has a per-user limit of 18 sub-jobs per array. tcsh bash #!/bin/tcsh ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX #!/bin/bash ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX Tip If you need to include a job ID in a subsequent qsub command as in the following example, be sure to use quotation marks to preserve the [] brackets: `` qsub -W \"depend=afterok:317485[]\" postprocess.pbs ``` Batch script to run a command file (MPMD) job \u00b6 Multiple Program, Multiple Data (MPMD) jobs run multiple independent, sequential executables simultaneously. The executable commands appear in the command file ( cmdfile ) on separate lines. The command file, the executable files, and the input files should reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full pathnames in both your command file and job scripts. The command file used in the example job scripts has these four lines. ./cmd1.exe < input1 > output1 ./cmd2.exe < input2 > output2 ./cmd3.exe < input3 > output3 ./cmd4.exe < input4 > output4 The job will produce output files that reside in the directory in which the job was submitted. In place of executables, you can specify independent shell scripts, MATLAB scripts, or others, or you can mix and match executables with scripts. Each task should execute in about the same wall-clock time as the others. Tip If any of your command file lines invoke a utility such as IDL, MATLAB, NCL, R and so on, invoke it in batch mode rather than interactive mode or your job will hang until it reaches the specified walltime limit. See the user guide for the utility for how to invoke it in batch mode. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. setenv MPI_SHEPHERD true mpiexec_mpt launch_cf.sh cmdfile #!/bin/bash #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. export MPI_SHEPHERD = true mpiexec_mpt launch_cf.sh cmdfile Not a complete copy of : https://arc.ucar.edu/knowledge_base/72581486","title":"Cheyenne job script examples"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#cheyenne-job-script-examples","text":"When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting TMPDIR in your batch scripts as recommended here: Storing temporary files with TMPDIR . Specify an ompthreads value in your script to help ensure that any parallel job will run properly. This is particularly important in a couple of cases: If your code was compiled with the GNU or Intel compiler using the -f openmp option. If your code was compiled with a PGI compiler with option -mp . Tip For a parallel job that does not use OpenMP threads \u2013 for a pure MPI job, for example \u2013 specify ompthreads=1 in your PBS select statement as shown below. Failure to do so may result in the job oversubscribing its nodes, resulting in poor performance or puzzling behavior such as exceeding its wallclock limit. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel mpt If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/19.1.1 mpt/2.25 These examples are based on the following assumptions: You are using HPE's Message Passing Toolkit (MPT) MPI library. The programs being run were compiled with Intel 16.0.3 or a later version. Contact the NCAR Research Computing help desk for assistance with adapting them for other cases. When your script is ready, submit your batch job for scheduling as shown here .","title":"Cheyenne job script examples"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#page-contents","text":"Batch script to run an MPI job Batch script to run a pure OpenMP job Batch script to run a hybrid MPI/OpenMP job Batch script to run a job array Batch script to run a command file (MPMD) job Pinning tasks/threads to CPUs Dependent jobs","title":"Page contents"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-an-mpi-job","text":"tcsh bash #!/bin/tcsh ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe #!/bin/bash ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe","title":"Batch script to run an MPI job"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-pure-openmp-job","text":"To run a pure OpenMP job, specify the number of CPUs you want from the node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name #!/bin/bash #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name","title":"Batch script to run a pure OpenMP job"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-hybrid-mpiopenmp-job","text":"If you want to run a hybrid MPI/OpenMP configuration where each node uses threaded parallelism while the nodes communicate with each other using MPI, activate NUMA mode and run using the MPI launcher. Specify the number of CPUs you want from each node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name","title":"Batch script to run a hybrid MPI/OpenMP job"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-job-array","text":"Job arrays are useful when you want to run the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. The elements in a job array are known as \"sub-jobs.\" Before submitting this batch script: Place files input.1 through input.18 in the same directory where you have the sequential cmd command. The batch job specifies 18 sub-jobs indexed 1-18 that will run in the \"share\" queue. The Nth sub-job uses file input.N to produce file output.N. The \"share\" queue is recommended for running job arrays of sequential sub-jobs, or parallel sub-jobs each having from two to nine tasks. The share queue has a per-user limit of 18 sub-jobs per array. tcsh bash #!/bin/tcsh ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX #!/bin/bash ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX Tip If you need to include a job ID in a subsequent qsub command as in the following example, be sure to use quotation marks to preserve the [] brackets: `` qsub -W \"depend=afterok:317485[]\" postprocess.pbs ```","title":"Batch script to run a job array"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/#batch-script-to-run-a-command-file-mpmd-job","text":"Multiple Program, Multiple Data (MPMD) jobs run multiple independent, sequential executables simultaneously. The executable commands appear in the command file ( cmdfile ) on separate lines. The command file, the executable files, and the input files should reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full pathnames in both your command file and job scripts. The command file used in the example job scripts has these four lines. ./cmd1.exe < input1 > output1 ./cmd2.exe < input2 > output2 ./cmd3.exe < input3 > output3 ./cmd4.exe < input4 > output4 The job will produce output files that reside in the directory in which the job was submitted. In place of executables, you can specify independent shell scripts, MATLAB scripts, or others, or you can mix and match executables with scripts. Each task should execute in about the same wall-clock time as the others. Tip If any of your command file lines invoke a utility such as IDL, MATLAB, NCL, R and so on, invoke it in batch mode rather than interactive mode or your job will hang until it reaches the specified walltime limit. See the user guide for the utility for how to invoke it in batch mode. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. setenv MPI_SHEPHERD true mpiexec_mpt launch_cf.sh cmdfile #!/bin/bash #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. export MPI_SHEPHERD = true mpiexec_mpt launch_cf.sh cmdfile Not a complete copy of : https://arc.ucar.edu/knowledge_base/72581486","title":"Batch script to run a command file (MPMD) job"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/","text":"Cheyenne job script examples \u00b6 When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting TMPDIR in your batch scripts as recommended here: Storing temporary files with TMPDIR . Specify an ompthreads value in your script to help ensure that any parallel job will run properly. This is particularly important in a couple of cases: If your code was compiled with the GNU or Intel compiler using the -f openmp option. If your code was compiled with a PGI compiler with option -mp . Tip For a parallel job that does not use OpenMP threads \u2013 for a pure MPI job, for example \u2013 specify ompthreads=1 in your PBS select statement as shown below. Failure to do so may result in the job oversubscribing its nodes, resulting in poor performance or puzzling behavior such as exceeding its wallclock limit. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel mpt If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/19.1.1 mpt/2.25 These examples are based on the following assumptions: You are using HPE's Message Passing Toolkit (MPT) MPI library. The programs being run were compiled with Intel 16.0.3 or a later version. Contact the NCAR Research Computing help desk for assistance with adapting them for other cases. When your script is ready, submit your batch job for scheduling as shown here . Page contents \u00b6 Batch script to run an MPI job Batch script to run a pure OpenMP job Batch script to run a hybrid MPI/OpenMP job Batch script to run a job array Batch script to run a command file (MPMD) job Pinning tasks/threads to CPUs Dependent jobs Batch script to run an MPI job \u00b6 tcsh bash #!/bin/tcsh ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe #!/bin/bash ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe Batch script to run a pure OpenMP job \u00b6 To run a pure OpenMP job, specify the number of CPUs you want from the node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name #!/bin/bash #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name Batch script to run a hybrid MPI/OpenMP job \u00b6 If you want to run a hybrid MPI/OpenMP configuration where each node uses threaded parallelism while the nodes communicate with each other using MPI, activate NUMA mode and run using the MPI launcher. Specify the number of CPUs you want from each node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name Batch script to run a job array \u00b6 Job arrays are useful when you want to run the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. The elements in a job array are known as \"sub-jobs.\" Before submitting this batch script: Place files input.1 through input.18 in the same directory where you have the sequential cmd command. The batch job specifies 18 sub-jobs indexed 1-18 that will run in the \"share\" queue. The Nth sub-job uses file input.N to produce file output.N. The \"share\" queue is recommended for running job arrays of sequential sub-jobs, or parallel sub-jobs each having from two to nine tasks. The share queue has a per-user limit of 18 sub-jobs per array. tcsh bash #!/bin/tcsh ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX #!/bin/bash ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX Tip If you need to include a job ID in a subsequent qsub command as in the following example, be sure to use quotation marks to preserve the [] brackets: `` qsub -W \"depend=afterok:317485[]\" postprocess.pbs ``` Batch script to run a command file (MPMD) job \u00b6 Multiple Program, Multiple Data (MPMD) jobs run multiple independent, sequential executables simultaneously. The executable commands appear in the command file ( cmdfile ) on separate lines. The command file, the executable files, and the input files should reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full pathnames in both your command file and job scripts. The command file used in the example job scripts has these four lines. ./cmd1.exe < input1 > output1 ./cmd2.exe < input2 > output2 ./cmd3.exe < input3 > output3 ./cmd4.exe < input4 > output4 The job will produce output files that reside in the directory in which the job was submitted. In place of executables, you can specify independent shell scripts, MATLAB scripts, or others, or you can mix and match executables with scripts. Each task should execute in about the same wall-clock time as the others. Tip If any of your command file lines invoke a utility such as IDL, MATLAB, NCL, R and so on, invoke it in batch mode rather than interactive mode or your job will hang until it reaches the specified walltime limit. See the user guide for the utility for how to invoke it in batch mode. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. setenv MPI_SHEPHERD true mpiexec_mpt launch_cf.sh cmdfile #!/bin/bash #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. export MPI_SHEPHERD = true mpiexec_mpt launch_cf.sh cmdfile Not a complete copy of : https://arc.ucar.edu/knowledge_base/72581486","title":"Cheyenne job script examples"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/#cheyenne-job-script-examples","text":"When you use any of these examples, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting TMPDIR in your batch scripts as recommended here: Storing temporary files with TMPDIR . Specify an ompthreads value in your script to help ensure that any parallel job will run properly. This is particularly important in a couple of cases: If your code was compiled with the GNU or Intel compiler using the -f openmp option. If your code was compiled with a PGI compiler with option -mp . Tip For a parallel job that does not use OpenMP threads \u2013 for a pure MPI job, for example \u2013 specify ompthreads=1 in your PBS select statement as shown below. Failure to do so may result in the job oversubscribing its nodes, resulting in poor performance or puzzling behavior such as exceeding its wallclock limit. Load all modules that are necessary to run your program at the start of your batch scripts by including a line like this: module load intel mpt If you think you might run a particular compiled executable well into the future, we advise that you load specific versions of desired modules to ensure reproducibility. Follow this example: module load intel/19.1.1 mpt/2.25 These examples are based on the following assumptions: You are using HPE's Message Passing Toolkit (MPT) MPI library. The programs being run were compiled with Intel 16.0.3 or a later version. Contact the NCAR Research Computing help desk for assistance with adapting them for other cases. When your script is ready, submit your batch job for scheduling as shown here .","title":"Cheyenne job script examples"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/#page-contents","text":"Batch script to run an MPI job Batch script to run a pure OpenMP job Batch script to run a hybrid MPI/OpenMP job Batch script to run a job array Batch script to run a command file (MPMD) job Pinning tasks/threads to CPUs Dependent jobs","title":"Page contents"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/#batch-script-to-run-an-mpi-job","text":"tcsh bash #!/bin/tcsh ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe #!/bin/bash ### Job Name #PBS -N mpi_job ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q queue_name ### Merge output and error files #PBS -j oe #PBS -k eod ### Select 2 nodes with 36 CPUs each for a total of 72 MPI processes #PBS -l select=2:ncpus=36:mpiprocs=36:ompthreads=1 ### Send email on abort, begin and end #PBS -m abe ### Specify mail recipient #PBS -M email_address export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the executable mpiexec_mpt ./executable_name.exe","title":"Batch script to run an MPI job"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/#batch-script-to-run-a-pure-openmp-job","text":"To run a pure OpenMP job, specify the number of CPUs you want from the node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name #!/bin/bash #PBS -A project_code #PBS -N OpenMP_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request 10 CPUS for 10 threads #PBS -l select=1:ncpus=10:ompthreads=10 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run OpenMP program ./executable_name","title":"Batch script to run a pure OpenMP job"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/#batch-script-to-run-a-hybrid-mpiopenmp-job","text":"If you want to run a hybrid MPI/OpenMP configuration where each node uses threaded parallelism while the nodes communicate with each other using MPI, activate NUMA mode and run using the MPI launcher. Specify the number of CPUs you want from each node ( ncpus ). Also specify the number of threads ( ompthreads ) or OMP_NUM_THREADS will default to the value of ncpus, possibly resulting in poor performance. Warning You will be charged for use of all CPUs on the node when using an exclusive queue. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request two nodes, each with one MPI task and 36 threads #PBS -l select=2:ncpus=36:mpiprocs=1:ompthreads=36 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR ### Run the hybrid OpenMP/MPI program mpiexec_mpt omplace ./executable_name","title":"Batch script to run a hybrid MPI/OpenMP job"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/#batch-script-to-run-a-job-array","text":"Job arrays are useful when you want to run the same program repeatedly on different input files. PBS can process a job array more efficiently than it can process the same number of individual non-array jobs. The elements in a job array are known as \"sub-jobs.\" Before submitting this batch script: Place files input.1 through input.18 in the same directory where you have the sequential cmd command. The batch job specifies 18 sub-jobs indexed 1-18 that will run in the \"share\" queue. The Nth sub-job uses file input.N to produce file output.N. The \"share\" queue is recommended for running job arrays of sequential sub-jobs, or parallel sub-jobs each having from two to nine tasks. The share queue has a per-user limit of 18 sub-jobs per array. tcsh bash #!/bin/tcsh ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX #!/bin/bash ### Job Name #PBS -N job_arrays ### Project code #PBS -A project_code #PBS -l walltime=01:00:00 #PBS -q share ### Merge output and error files #PBS -j oe #PBS -k eod ### Select one CPU #PBS -l select=1:ncpus=1 ### Specify index range of sub-jobs #PBS -J 1-18 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Execute subjob for index PBS_ARRAY_INDEX cmd input. $PBS_ARRAY_INDEX > output. $PBS_ARRAY_INDEX Tip If you need to include a job ID in a subsequent qsub command as in the following example, be sure to use quotation marks to preserve the [] brackets: `` qsub -W \"depend=afterok:317485[]\" postprocess.pbs ```","title":"Batch script to run a job array"},{"location":"cheyenne/starting-cheyenne-jobs/cheyenne-job-script-examples/Cheyenne%2Bjob%2Bscript%2Bexamples/#batch-script-to-run-a-command-file-mpmd-job","text":"Multiple Program, Multiple Data (MPMD) jobs run multiple independent, sequential executables simultaneously. The executable commands appear in the command file ( cmdfile ) on separate lines. The command file, the executable files, and the input files should reside in the directory from which the job is submitted. If they don't, you need to specify adequate relative or full pathnames in both your command file and job scripts. The command file used in the example job scripts has these four lines. ./cmd1.exe < input1 > output1 ./cmd2.exe < input2 > output2 ./cmd3.exe < input3 > output3 ./cmd4.exe < input4 > output4 The job will produce output files that reside in the directory in which the job was submitted. In place of executables, you can specify independent shell scripts, MATLAB scripts, or others, or you can mix and match executables with scripts. Each task should execute in about the same wall-clock time as the others. Tip If any of your command file lines invoke a utility such as IDL, MATLAB, NCL, R and so on, invoke it in batch mode rather than interactive mode or your job will hang until it reaches the specified walltime limit. See the user guide for the utility for how to invoke it in batch mode. tcsh bash #!/bin/tcsh #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 setenv TMPDIR /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. setenv MPI_SHEPHERD true mpiexec_mpt launch_cf.sh cmdfile #!/bin/bash #PBS -A project_code #PBS -N cmd_file #PBS -j oe #PBS -k eod #PBS -m abe #PBS -M email_address #PBS -q queue_name #PBS -l walltime=01:00:00 ### Request one chunk with ncpus and mpiprocs set to ### the number of lines in the command file #PBS -l select=1:ncpus=4:mpiprocs=4:ompthreads=1 export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # yyyy-mm-dd Context: Cheyenne MPT command file job. # Do not propagate this use of MPI_SHEPHERD # to other MPT jobs as it may cause # significant slowdown or timeout. # Contact the CISL Consulting Services Group # if you have questions about this. export MPI_SHEPHERD = true mpiexec_mpt launch_cf.sh cmdfile Not a complete copy of : https://arc.ucar.edu/knowledge_base/72581486","title":"Batch script to run a command file (MPMD) job"},{"location":"derecho/","text":"Derecho \u00b6 Derecho, the new supercomputer NCAR installed in 2023, features 2,488 compute nodes with 128 AMD Milan cores per node and 82 nodes with four NVIDIA A100 GPUs each. The HPE Cray EX cluster is a 19.87-petaflops system that is expected to deliver about 3.5 times the scientific throughput of the Cheyenne system. Additional hardware details are available below. See the following pages for user documentation that is relevant to all NCAR systems (compiling code, environment module basics, managing allocations), and the menu on the right of your screen for system-specific information. Getting started with NCAR systems New user orientation User support Additional Derecho documentation is in development. Estimating Derecho Allocation Needs Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems. Derecho Hardware \u00b6 323,712 processor cores 3 rd Gen AMD EPYC\u2122 7763 Milan processors 2,488 CPU-only computation nodes Dual-socket nodes, 64 cores per socket 256 GB DDR4 memory per node 82 GPU nodes Single-socket nodes, 64 cores per socket 512 GB DDR4 memory per node 4 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 600 GB/s NVIDIA NVLink GPU interconnect 328 total A100 GPUs 40GB HBM2 memory per GPU 600 GB/s NVIDIA NVLink GPU interconnect 6 CPU login nodes Dual-socket nodes with AMD EPYC\u2122 7763 Milan CPUs 64 cores per socket 512 GB DDR4-3200 memory 2 GPU development and testing nodes Dual-socket nodes with AMD EPYC\u2122 7543 Milan CPUs 32 cores per socket 2 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 512 GB DDR4-3200 memory 692 TB total system memory 637 TB DDR4 memory on 2,488 CPU nodes 42 TB DDR4 memory on 82 GPU nodes 13 TB HBM2 memory on 82 GPU nodes HPE Slingshot v11 high-speed interconnect Dragonfly topology, 200 Gb/sec per port per direction 1.7-2.6 usec MPI latency CPU-only nodes - one Slingshot injection port GPU nodes - 4 Slingshot injection ports per node ~3.5 times Cheyenne computational capacity Comparison based on the relative performance of CISL's High Performance Computing Benchmarks run on each system. > 3.5 times Cheyenne peak performance 19.87 peak petaflops (vs 5.34) Just proof of concept for https://arc.ucar.edu/knowledge_base/74317833","title":"Derecho"},{"location":"derecho/#derecho","text":"Derecho, the new supercomputer NCAR installed in 2023, features 2,488 compute nodes with 128 AMD Milan cores per node and 82 nodes with four NVIDIA A100 GPUs each. The HPE Cray EX cluster is a 19.87-petaflops system that is expected to deliver about 3.5 times the scientific throughput of the Cheyenne system. Additional hardware details are available below. See the following pages for user documentation that is relevant to all NCAR systems (compiling code, environment module basics, managing allocations), and the menu on the right of your screen for system-specific information. Getting started with NCAR systems New user orientation User support Additional Derecho documentation is in development. Estimating Derecho Allocation Needs Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems.","title":"Derecho"},{"location":"derecho/#derecho-hardware","text":"323,712 processor cores 3 rd Gen AMD EPYC\u2122 7763 Milan processors 2,488 CPU-only computation nodes Dual-socket nodes, 64 cores per socket 256 GB DDR4 memory per node 82 GPU nodes Single-socket nodes, 64 cores per socket 512 GB DDR4 memory per node 4 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 600 GB/s NVIDIA NVLink GPU interconnect 328 total A100 GPUs 40GB HBM2 memory per GPU 600 GB/s NVIDIA NVLink GPU interconnect 6 CPU login nodes Dual-socket nodes with AMD EPYC\u2122 7763 Milan CPUs 64 cores per socket 512 GB DDR4-3200 memory 2 GPU development and testing nodes Dual-socket nodes with AMD EPYC\u2122 7543 Milan CPUs 32 cores per socket 2 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 512 GB DDR4-3200 memory 692 TB total system memory 637 TB DDR4 memory on 2,488 CPU nodes 42 TB DDR4 memory on 82 GPU nodes 13 TB HBM2 memory on 82 GPU nodes HPE Slingshot v11 high-speed interconnect Dragonfly topology, 200 Gb/sec per port per direction 1.7-2.6 usec MPI latency CPU-only nodes - one Slingshot injection port GPU nodes - 4 Slingshot injection ports per node ~3.5 times Cheyenne computational capacity Comparison based on the relative performance of CISL's High Performance Computing Benchmarks run on each system. > 3.5 times Cheyenne peak performance 19.87 peak petaflops (vs 5.34) Just proof of concept for https://arc.ucar.edu/knowledge_base/74317833","title":"Derecho Hardware"},{"location":"derecho/Batch%2Bjob%2Bscript%2Bexamples-Derecho/","text":"Batch job script examples - Derecho \u00b6 When using these examples to create your own job scripts to run on Derecho, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting your TMPDIR environment variable as recommended here: Storing temporary files with TMPDIR . Page contents \u00b6 Running a hybrid CPU program with MPI and OpenMP Running an MPI-enabled GPU application Binding MPI ranks to CPU cores and GPU devices Running a hybrid CPU program with MPI and OpenMP \u00b6 In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank. Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the OMP_NUM_THREADS environment variable. #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Load modules to match compile-time environment module purge module load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21 # Run application using cray-mpich with binding mpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name Running an MPI-enabled GPU application \u00b6 In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and cray-mpich MPI. We request all four GPUs on each of two nodes. Please ensure that you have the cuda module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive. #!/bin/bash #PBS -A project_code #PBS -N gpu_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Load modules to match compile-time environment module purge module load nvhpc cuda cray-mpich # (Optional: Enable GPU managed memory if required.) # From \u2018man mpi\u2019: This setting will allow MPI to properly # handle unify memory addresses. This setting has performance # penalties as MPICH will perform buffer query on each buffer # that is handled by MPI) # If you see runtime errors like # (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, # CUDA_ERROR_INVALID_VALUE # make sure this variable is set export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 # Run application using the cray-mpich MPI # The \u2018get_local_rank\u2019 command is a script that sets several GPU- # related environment variables to allow MPI-enabled GPU # applications to run. The get_local_rank script is detailed # in the binding section below, and is also made available # via the ncarenv module. mpiexec -n 8 -ppn 4 get_local_rank ./executable_name Binding MPI ranks to CPU cores and GPU devices \u00b6 For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the CUDA_VISIBLE_DEVICES environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node. Consider the following shell script: get_local_rank #!/bin/bash export MPICH_GPU_SUPPORT_ENABLED = 1 export LOCAL_RANK = ${ PMI_LOCAL_RANK } export GLOBAL_RANK = ${ PMI_RANK } export CUDA_VISIBLE_DEVICES = $( expr ${ LOCAL_RANK } % 4 ) echo \"Global Rank ${ GLOBAL_RANK } / Local Rank ${ LOCAL_RANK } / CUDA_VISIBLE_DEVICES= ${ CUDA_VISIBLE_DEVICES } / $( hostname ) \" exec $* It can be used underneath mpiexec to bind an MPI process to a particular GPU: #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 ... # Run application using the cray-mpich MPI, binding the local # mpi rank [0-3] to corresponding GPU index [0-3]: mpiexec -n 8 -ppn 4 ./get_local_rank ./executable_name The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see man mpi for other examples and scenarios. Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the --cpu-bind option to mpiexec . For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./get_local_rank ./executable_name","title":"Batch job script examples - Derecho"},{"location":"derecho/Batch%2Bjob%2Bscript%2Bexamples-Derecho/#batch-job-script-examples-derecho","text":"When using these examples to create your own job scripts to run on Derecho, remember to substitute your own job name and project code, and customize the other directives and commands as necessary. That includes the commands shown for setting your TMPDIR environment variable as recommended here: Storing temporary files with TMPDIR .","title":"Batch job script examples - Derecho"},{"location":"derecho/Batch%2Bjob%2Bscript%2Bexamples-Derecho/#page-contents","text":"Running a hybrid CPU program with MPI and OpenMP Running an MPI-enabled GPU application Binding MPI ranks to CPU cores and GPU devices","title":"Page contents"},{"location":"derecho/Batch%2Bjob%2Bscript%2Bexamples-Derecho/#running-a-hybrid-cpu-program-with-mpi-and-openmp","text":"In this example, we run a hybrid application that uses both MPI tasks and OpenMP threads. The executable was compiled using default modules (Intel compilers and MPI). We use a 2 nodes with 32 MPI ranks on each node and 4 OpenMP threads per MPI rank. Whenever you run a program that compiled with OpenMP support, it is important to provide a value for ompthreads in the select statement; PBS will use that value to define the OMP_NUM_THREADS environment variable. #!/bin/bash #PBS -A project_code #PBS -N hybrid_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=128:mpiprocs=32:ompthreads=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Load modules to match compile-time environment module purge module load ncarenv/22.12 oneapi/2022.2.1 craype/2.7.19 cray-mpich/8.1.21 # Run application using cray-mpich with binding mpiexec --cpu-bind depth -n 64 -ppn 32 -d 4 ./executable_name","title":"Running a hybrid CPU program with MPI and OpenMP"},{"location":"derecho/Batch%2Bjob%2Bscript%2Bexamples-Derecho/#running-an-mpi-enabled-gpu-application","text":"In this example, we run an MPI CUDA program. The application was compiled using the NVIDIA HPC SDK compilers, the CUDA toolkit, and cray-mpich MPI. We request all four GPUs on each of two nodes. Please ensure that you have the cuda module loaded as shown below when attempting to run GPU applications or nodes may lock up and become unresponsive. #!/bin/bash #PBS -A project_code #PBS -N gpu_job #PBS -q main #PBS -l walltime=01:00:00 #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 # Use scratch for temporary files to avoid space limits in /tmp export TMPDIR = /glade/scratch/ $USER /temp mkdir -p $TMPDIR # Load modules to match compile-time environment module purge module load nvhpc cuda cray-mpich # (Optional: Enable GPU managed memory if required.) # From \u2018man mpi\u2019: This setting will allow MPI to properly # handle unify memory addresses. This setting has performance # penalties as MPICH will perform buffer query on each buffer # that is handled by MPI) # If you see runtime errors like # (GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, # CUDA_ERROR_INVALID_VALUE # make sure this variable is set export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED = 1 # Run application using the cray-mpich MPI # The \u2018get_local_rank\u2019 command is a script that sets several GPU- # related environment variables to allow MPI-enabled GPU # applications to run. The get_local_rank script is detailed # in the binding section below, and is also made available # via the ncarenv module. mpiexec -n 8 -ppn 4 get_local_rank ./executable_name","title":"Running an MPI-enabled GPU application"},{"location":"derecho/Batch%2Bjob%2Bscript%2Bexamples-Derecho/#binding-mpi-ranks-to-cpu-cores-and-gpu-devices","text":"For some GPU applications, you may need to explicitly control the mapping between MPI ranks and GPU devices (see man mpi). One approach is to manually control the CUDA_VISIBLE_DEVICES environment variable so a given MPI rank only \u201csees\u201d a subset of the GPU devices on a node. Consider the following shell script: get_local_rank #!/bin/bash export MPICH_GPU_SUPPORT_ENABLED = 1 export LOCAL_RANK = ${ PMI_LOCAL_RANK } export GLOBAL_RANK = ${ PMI_RANK } export CUDA_VISIBLE_DEVICES = $( expr ${ LOCAL_RANK } % 4 ) echo \"Global Rank ${ GLOBAL_RANK } / Local Rank ${ LOCAL_RANK } / CUDA_VISIBLE_DEVICES= ${ CUDA_VISIBLE_DEVICES } / $( hostname ) \" exec $* It can be used underneath mpiexec to bind an MPI process to a particular GPU: #PBS -l select=2:ncpus=64:mpiprocs=4:ngpus=4 ... # Run application using the cray-mpich MPI, binding the local # mpi rank [0-3] to corresponding GPU index [0-3]: mpiexec -n 8 -ppn 4 ./get_local_rank ./executable_name The command above will launch a total of 8 MPI ranks across 2 nodes, using 4 MPI ranks per node, and each rank will have dedicated access to one of the 4 GPUs on the node. Again, see man mpi for other examples and scenarios. Binding MPI ranks to CPU cores can also be an important performance consideration for GPU-enabled codes, and can be done with the --cpu-bind option to mpiexec . For the above example using 2 nodes, 4 MPI ranks per node, and 1 GPU per MPI rank, binding each of the MPI ranks to one of the four separate NUMA domains within a node is likely to be optimal for performance. This could be done as follows: mpiexec -n 8 -ppn 4 --cpu-bind verbose,list:0:16:32:48 ./get_local_rank ./executable_name","title":"Binding MPI ranks to CPU cores and GPU devices"},{"location":"derecho/derecho/","text":"Derecho \u00b6 Derecho, the new supercomputer NCAR installed in 2023, features 2,488 compute nodes with 128 AMD Milan cores per node and 82 nodes with four NVIDIA A100 GPUs each. The HPE Cray EX cluster is a 19.87-petaflops system that is expected to deliver about 3.5 times the scientific throughput of the Cheyenne system. Additional hardware details are available below. See the following pages for user documentation that is relevant to all NCAR systems (compiling code, environment module basics, managing allocations), and the menu on the right of your screen for system-specific information. Getting started with NCAR systems New user orientation User support Additional Derecho documentation is in development. Estimating Derecho Allocation Needs Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems. Derecho Hardware \u00b6 323,712 processor cores 3 rd Gen AMD EPYC\u2122 7763 Milan processors 2,488 CPU-only computation nodes Dual-socket nodes, 64 cores per socket 256 GB DDR4 memory per node 82 GPU nodes Single-socket nodes, 64 cores per socket 512 GB DDR4 memory per node 4 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 600 GB/s NVIDIA NVLink GPU interconnect 328 total A100 GPUs 40GB HBM2 memory per GPU 600 GB/s NVIDIA NVLink GPU interconnect 6 CPU login nodes Dual-socket nodes with AMD EPYC\u2122 7763 Milan CPUs 64 cores per socket 512 GB DDR4-3200 memory 2 GPU development and testing nodes Dual-socket nodes with AMD EPYC\u2122 7543 Milan CPUs 32 cores per socket 2 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 512 GB DDR4-3200 memory 692 TB total system memory 637 TB DDR4 memory on 2,488 CPU nodes 42 TB DDR4 memory on 82 GPU nodes 13 TB HBM2 memory on 82 GPU nodes HPE Slingshot v11 high-speed interconnect Dragonfly topology, 200 Gb/sec per port per direction 1.7-2.6 usec MPI latency CPU-only nodes - one Slingshot injection port GPU nodes - 4 Slingshot injection ports per node ~3.5 times Cheyenne computational capacity Comparison based on the relative performance of CISL's High Performance Computing Benchmarks run on each system. > 3.5 times Cheyenne peak performance 19.87 peak petaflops (vs 5.34) Just proof of concept for https://arc.ucar.edu/knowledge_base/74317833","title":"Derecho"},{"location":"derecho/derecho/#derecho","text":"Derecho, the new supercomputer NCAR installed in 2023, features 2,488 compute nodes with 128 AMD Milan cores per node and 82 nodes with four NVIDIA A100 GPUs each. The HPE Cray EX cluster is a 19.87-petaflops system that is expected to deliver about 3.5 times the scientific throughput of the Cheyenne system. Additional hardware details are available below. See the following pages for user documentation that is relevant to all NCAR systems (compiling code, environment module basics, managing allocations), and the menu on the right of your screen for system-specific information. Getting started with NCAR systems New user orientation User support Additional Derecho documentation is in development. Estimating Derecho Allocation Needs Derecho users can expect to see a 1.3x improvement over the Cheyenne system's performance on a core-for-core basis. Therefore, to estimate how many CPU core-hours will be needed for a project on Derecho, multiply the total for a Cheyenne project by 0.77 . When requesting an allocation for Derecho GPU nodes, please make your request in terms of GPU-hours (number of GPUs used x wallclock hours). We encourage researchers to estimate GPU-hour needs by making test/benchmark runs on Casper GPUs, but will accept estimates based on runs on comparable non-NCAR, GPU-based systems.","title":"Derecho"},{"location":"derecho/derecho/#derecho-hardware","text":"323,712 processor cores 3 rd Gen AMD EPYC\u2122 7763 Milan processors 2,488 CPU-only computation nodes Dual-socket nodes, 64 cores per socket 256 GB DDR4 memory per node 82 GPU nodes Single-socket nodes, 64 cores per socket 512 GB DDR4 memory per node 4 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 600 GB/s NVIDIA NVLink GPU interconnect 328 total A100 GPUs 40GB HBM2 memory per GPU 600 GB/s NVIDIA NVLink GPU interconnect 6 CPU login nodes Dual-socket nodes with AMD EPYC\u2122 7763 Milan CPUs 64 cores per socket 512 GB DDR4-3200 memory 2 GPU development and testing nodes Dual-socket nodes with AMD EPYC\u2122 7543 Milan CPUs 32 cores per socket 2 NVIDIA 1.41 GHz A100 Tensor Core GPUs per node 512 GB DDR4-3200 memory 692 TB total system memory 637 TB DDR4 memory on 2,488 CPU nodes 42 TB DDR4 memory on 82 GPU nodes 13 TB HBM2 memory on 82 GPU nodes HPE Slingshot v11 high-speed interconnect Dragonfly topology, 200 Gb/sec per port per direction 1.7-2.6 usec MPI latency CPU-only nodes - one Slingshot injection port GPU nodes - 4 Slingshot injection ports per node ~3.5 times Cheyenne computational capacity Comparison based on the relative performance of CISL's High Performance Computing Benchmarks run on each system. > 3.5 times Cheyenne peak performance 19.87 peak petaflops (vs 5.34) Just proof of concept for https://arc.ucar.edu/knowledge_base/74317833","title":"Derecho Hardware"},{"location":"esds/faq/","text":"Frequently Asked Questions \u00b6 A copy of ESDS FAQ page: https://ncar.github.io/esds/faq/ This contains relevant questions and answers from common workflow issues and questions posted on Zulip. Note This page is meant to be a list of FAQ regarding climate datasets, movivated by a variety of employees across UCAR/NCAR. I need help with this! \u00b6 Where do I go for help? \u00b6 Try one of the following resources. Xarray's How Do I do X? page Xarray Github Discussions Pangeo Discourse Forum NCAR Zulip under #python-questions, #python-dev, or #dask. Avoid personal emails and prefer a public forum. What do I do if my question is not answered on this page? \u00b6 If your question is related to conda environments and you're affiliated with UCAR/NCAR, you can open a help ticket on the NCAR Research Computing Helpdesk site . If your issue is related to data science packages and workflows, you can open an issue on our GitHub here or book an office hour appointment with an ESDS core member! Someone must have written the function I want. Where do I look? \u00b6 See the xarray ecosystem page. Also see the xarray-contrib and pangeo-data organizations. Some NCAR relevant projects include: GeoCAT-comp GeoCAT-viz cf_xarray climpred eofs MetPy rechunker xclim xesmf xgcm pop-tools xskillscore How do I use conda environments? \u00b6 General Advice \u00b6 Dealing with Python environments can be tricky... a good place to start is to checkout this guide on dealing with Python environments . If you just need a refresher on the various conda commands, this conda cheet sheet is a wonderful quick reference. Using conda on NCAR HPC resources \u00b6 Warning Since 12 December 2022, it is no longer recommended to install your own version of miniconda on the HPC system. To export your existing environments to the recommended installation of miniconda, refer to the \"How can I export my environments?\" section. The NCAR High Performance Computing (HPC) system has a conda installation for you to use. The most recent and detailed instructions can be found on this Using Conda and Python page. If you don't want the trouble of making your own conda environment, there are managed environments available. The NCAR Package Library (NPL) is an environment containing many common scientific Python pacakges such as Numpy, Xarray, and GeoCAT. You can access the NPL environment through the command line and the NCAR JupyterHub. NPL on the command line \u00b6 Open up a terminal in Casper or Cheyenne Load the NCAR conda module: $ module load conda/latest List the available NCAR managed environments: $ conda env list base * /glade/u/apps/opt/conda npl /glade/u/apps/opt/conda/envs/npl npl-2022b /glade/u/apps/opt/conda/envs/npl-2022b npl-2206 /glade/u/apps/opt/conda/envs/npl-2206 npl-2207 /glade/u/apps/opt/conda/envs/npl-2207 pygpu-dask /glade/u/apps/opt/conda/envs/pygpu-dask Activate the environment you want to use. Here we are using the npl environment as an example. npl can be replaced with any available environment name: $ conda activate npl Now when you run a script, the modules within the npl environment will be available to your program. NPL on the NCAR JupyterHub \u00b6 Log in to the Production NCAR JupyterHub Start a server With your Jupyter Notebook open, click on the kernel name in the upper right. A dialog will appear with all the verious kernels available to you. These kernels will (generally) have the same name as the conda environment that it uses. This may not be the case if you are managing your own environments and kernels. Select the \"npl (conda)\" kernel from the list if you want to use the NCAR-managed NPL environment. Creating and accessing a new conda environment on the NCAR JupyterHub \u00b6 You may want to move past using NPL, and create a new conda environment! For detailed instructions, check out the Using Conda and Python page on the NCAR Advanced Research Computing site. Heres a summary of the basic steps: Create the environment If you are creating an environment from scratch, use the following: conda create --name my_environment where my_environment is the name of your environment Ff you have an environment file (ex. environment.yml ), use the following: conda env create -f environment.yml Activate your environment and install the ipykernel package conda activate my_environment.yml conda install ipykernel The [`ipykernel`](https://github.com/ipython/ipykernel) package is required for your environment to be available from the NCAR [JupyterHub](https://jupyterhub.hpc.ucar.edu/) Accessing your conda environment Your environment should now automatically show up as an available kernel in any Jupyter server on the NCAR HPC systems. If you want to give your kernel a name that is different from the environment name, you can use the following command: python -m ipykernel install --user --name = my-kernel Where my-kernel is the kernel name. Conda is taking too long to solve environment: use mamba \u00b6 This is a very common issue when installing a new package or trying to update a package in an existing conda environment. This issue is usually manifested in a conda message along these lines: environment Solving environment: failed with initial frozen solve. Retrying with flexible solve. One solution to this issue is to use mamba which is a drop-in replacement for conda. Mamba aims to greately speed up and improve conda functionality such as solving environment, installing packages, etc... Installing Mamba conda install -n base -c conda-forge mamba Set conda-forge and nodefaults channels conda config --add channels nodefaults conda config --add channels conda-forge To install a package with mamba, you just run mamba install package_name To create/update an environment from an environment file, run: mamba env update -f environment.yml We do not recommend using `mamba` to activate and deactivate environments as this can cause packages to misbehave/not load correctly. See mamba documentation for more. How can I export my environments? \u00b6 If you made an environment on one machine or using a different conda installation, you can export that environment and use it elsewhere. These are the basic steps: Export your environment With the environment you want to export activated, run the following command: conda env export --from-history > environment.yml where environment can be replaced with the file name of your choice. The --from-history flag allows you to recreate your environment on any system. It is the cross-platform compatible way of exporting an environment. Move the environment.yml to the system you want to use it on / activate the appropriate conda installtion you wish to use. Use the .yml file to create your environment conda env create -f environment.yml Xarray and Dask \u00b6 General tips \u00b6 Read the xarray documentation on optimizing workflow with dask . Read the Best practices for dask array Keep track of chunk sizes throughout your workflow. This is especially important when reading in data using xr.open_mfdataset . Aim for 100-200MB size chunks. Choose chunking appropriate to your analysis. If you're working with time series then chunk more in space and less along time. Avoid indexing with .where as much as possible. In particulate .where(..., drop=True) will trigger a compute since it needs to know where NaNs are present to drop them. Instead see if you can write your statement as a .clip , .sel , .isel , or .query statement. How do I optimize reading multiple files using Xarray and Dask? \u00b6 A good first place to start when reading in multiple files is Xarray's multi-file documentation . For example, if you are trying to read in multiple files where you are interested in concatenating over the time dimension, here is an example of the xr.open_dataset line would look like: ds = xr . open_mfdataset ( files , # Name of the dimension to concatenate along. concat_dim = \"time\" , # Attempt to auto-magically combine the given datasets into one by using dimension coordinates. combine = \"by_coords\" , # Specify chunks for dask - explained later chunks = { \"lev\" : 1 , \"time\" : 500 }, # Only data variables in which the dimension already appears are included. data_vars = \"minimal\" , # Only coordinates in which the dimension already appears are included. coords = \"minimal\" , # Skip comparing and pick variable from first dataset. compat = \"override\" , parallel = True , ) Where can I find Xarray tutorials? \u00b6 See videos and notebooks . How do I debug my code when using dask? \u00b6 An option is to use .compute(scheduler=\"single-threaded\") . This will run your code as a serial for loop. When an error is raised you can use the %debug magic to drop in to the stack and debug from there. See this post for more debugging tips in a serial context. KilledWorker X{. What do I do? \u00b6 Keep an eye on the dask dashboard. If a lot of the bars in the Memory tab are orange, that means your workers are running out of memory. Reduce your chunk size. Help, my analysis is slow! \u00b6 Try subsetting for just the variable(s) you need for example, if you are reading in a dataset with ~25 variables, and you only need temperature , just read in temperature. You can specificy which variables to read in by using the following syntax, following the example of the temperature variable. ds = xr . open_dataset ( file , data_vars = [ 'temperature' ]) Take a look at your chunk size, it might not be optimized. When reading a file in using Xarray with Dask, a \"general rule of thumb\" is to keep your chunk size down to around 100 mb. For example, let's say you trying to read in multiple files, each with ~600 time steps. This is case where each file is very large (several 10s of GB) and using Dask to help with data processing is essential . You can check the size of each chunk by subsetting a single DataArray (ex. ds['temperature'] ) If you have very large chunks, try modifying the number of chunks you specify within xr.open_mfdataset(files, ..., chunks={'lev':1, \"time\": 500}) where lev and time are vertical and time dimensions respectively. Check to see how large each chunk is after modifying the chunk size, and modify as necessary. You do not have enough dask workers If you have a few large files, having the number of workers equal to to the number of input files read in using xr.open_mfdataset would be a good practice If you have a large number of smaller files, you may not run into this issue, and it is suggest you look at the other potential solutions. I have to do lots of rechunking, but the rechunk step uses too much memory and kills my workers. \u00b6 Try the rechunker package . Writing to files in parallel \u00b6 Distributed writes to netCDF are hard. Try writing to zarr using Dataset.to_zarr . If you need to write to netCDF and your final dataset can fit in memory then use dataset.load().to_netcdf(...) . If you really must write a big dataset to netCDF try using save_mfdataset (see here ). My Dask workers are taking a long time to start. How can I monitor them? \u00b6 Dask worker requests are added to the job queues on Casper and Cheyenne with the cluster.scale() method. After this method is called, you can verify that they are waiting in the queue with this command: qstat -u <my_username> on Cheyenne, and the same command will work on Casper after April 2021. If you see no pending worker jobs, then verify that you have called cluster.scale() . Github \u00b6 Setting up Github Authentication \u00b6 Beginning August 13, 2021, Github will no longer accept account passwords when authenticating git operations. There are essentially two options, which Github provides proper documentation for getting setup: Setup two-factor authentication Connect to Github via SSH CESM Data \u00b6 Dealing with CESM monthly output - is there something wrong with time \u00b6 A well known issue of CESM data is that timestamps for fields saved as averages are placed at the end of the averaging period. For instance, in the following example, the January/1920 average has a timestamp of February/1920 : In [ 25 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 33 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: ds . time Out [ 34 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 2 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 4 , 1 , 0 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 11 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2006 , 1 , 1 , 0 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 02 - 01 00 : 00 : 00 ... 2006 - 01 - 01 00 : 00 : 00 Attributes : long_name : time bounds : time_bnds A temporary workaround is to fix the issue ourselves by computing new time axis by averaging the time bounds: In [ 29 ]: import xarray as xr In [ 30 ]: import cf_xarray # use cf-xarray so that we can use CF attributes In [ 31 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 32 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: attrs , encoding = ds . time . attrs . copy (), ds . time . encoding . copy () In [ 36 ]: time_bounds = ds . cf . get_bounds ( 'time' ) In [ 37 ]: time_bounds_dim_name = ds . cf . get_bounds_dim_name ( 'time' ) In [ 38 ]: ds = ds . assign_coords ( time = time_bounds . mean ( time_bounds_dim_name )) In [ 39 ]: ds . time . attrs , ds . time . encoding = attrs , encoding In [ 40 ]: ds . time Out [ 40 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 1 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 2 , 15 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 16 , 12 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 10 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 11 , 16 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 16 , 12 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 01 - 16 12 : 00 : 00 ... 2005 - 12 - 16 12 : 00 : 00 Attributes : long_name : time bounds : time_bnds cf-xarray can be installed via pip or conda. cf-xarray docs are available [here](https://cf-xarray.readthedocs.io/en/latest/).","title":"Frequently Asked Questions"},{"location":"esds/faq/#frequently-asked-questions","text":"A copy of ESDS FAQ page: https://ncar.github.io/esds/faq/ This contains relevant questions and answers from common workflow issues and questions posted on Zulip. Note This page is meant to be a list of FAQ regarding climate datasets, movivated by a variety of employees across UCAR/NCAR.","title":"Frequently Asked Questions"},{"location":"esds/faq/#i-need-help-with-this","text":"","title":"I need help with this!"},{"location":"esds/faq/#where-do-i-go-for-help","text":"Try one of the following resources. Xarray's How Do I do X? page Xarray Github Discussions Pangeo Discourse Forum NCAR Zulip under #python-questions, #python-dev, or #dask. Avoid personal emails and prefer a public forum.","title":"Where do I go for help?"},{"location":"esds/faq/#what-do-i-do-if-my-question-is-not-answered-on-this-page","text":"If your question is related to conda environments and you're affiliated with UCAR/NCAR, you can open a help ticket on the NCAR Research Computing Helpdesk site . If your issue is related to data science packages and workflows, you can open an issue on our GitHub here or book an office hour appointment with an ESDS core member!","title":"What do I do if my question is not answered on this page?"},{"location":"esds/faq/#someone-must-have-written-the-function-i-want-where-do-i-look","text":"See the xarray ecosystem page. Also see the xarray-contrib and pangeo-data organizations. Some NCAR relevant projects include: GeoCAT-comp GeoCAT-viz cf_xarray climpred eofs MetPy rechunker xclim xesmf xgcm pop-tools xskillscore","title":"Someone must have written the function I want. Where do I look?"},{"location":"esds/faq/#how-do-i-use-conda-environments","text":"","title":"How do I use conda environments?"},{"location":"esds/faq/#general-advice","text":"Dealing with Python environments can be tricky... a good place to start is to checkout this guide on dealing with Python environments . If you just need a refresher on the various conda commands, this conda cheet sheet is a wonderful quick reference.","title":"General Advice"},{"location":"esds/faq/#using-conda-on-ncar-hpc-resources","text":"Warning Since 12 December 2022, it is no longer recommended to install your own version of miniconda on the HPC system. To export your existing environments to the recommended installation of miniconda, refer to the \"How can I export my environments?\" section. The NCAR High Performance Computing (HPC) system has a conda installation for you to use. The most recent and detailed instructions can be found on this Using Conda and Python page. If you don't want the trouble of making your own conda environment, there are managed environments available. The NCAR Package Library (NPL) is an environment containing many common scientific Python pacakges such as Numpy, Xarray, and GeoCAT. You can access the NPL environment through the command line and the NCAR JupyterHub.","title":"Using conda on NCAR HPC resources"},{"location":"esds/faq/#npl-on-the-command-line","text":"Open up a terminal in Casper or Cheyenne Load the NCAR conda module: $ module load conda/latest List the available NCAR managed environments: $ conda env list base * /glade/u/apps/opt/conda npl /glade/u/apps/opt/conda/envs/npl npl-2022b /glade/u/apps/opt/conda/envs/npl-2022b npl-2206 /glade/u/apps/opt/conda/envs/npl-2206 npl-2207 /glade/u/apps/opt/conda/envs/npl-2207 pygpu-dask /glade/u/apps/opt/conda/envs/pygpu-dask Activate the environment you want to use. Here we are using the npl environment as an example. npl can be replaced with any available environment name: $ conda activate npl Now when you run a script, the modules within the npl environment will be available to your program.","title":"NPL on the command line"},{"location":"esds/faq/#npl-on-the-ncar-jupyterhub","text":"Log in to the Production NCAR JupyterHub Start a server With your Jupyter Notebook open, click on the kernel name in the upper right. A dialog will appear with all the verious kernels available to you. These kernels will (generally) have the same name as the conda environment that it uses. This may not be the case if you are managing your own environments and kernels. Select the \"npl (conda)\" kernel from the list if you want to use the NCAR-managed NPL environment.","title":"NPL on the NCAR JupyterHub"},{"location":"esds/faq/#creating-and-accessing-a-new-conda-environment-on-the-ncar-jupyterhub","text":"You may want to move past using NPL, and create a new conda environment! For detailed instructions, check out the Using Conda and Python page on the NCAR Advanced Research Computing site. Heres a summary of the basic steps: Create the environment If you are creating an environment from scratch, use the following: conda create --name my_environment where my_environment is the name of your environment Ff you have an environment file (ex. environment.yml ), use the following: conda env create -f environment.yml Activate your environment and install the ipykernel package conda activate my_environment.yml conda install ipykernel The [`ipykernel`](https://github.com/ipython/ipykernel) package is required for your environment to be available from the NCAR [JupyterHub](https://jupyterhub.hpc.ucar.edu/) Accessing your conda environment Your environment should now automatically show up as an available kernel in any Jupyter server on the NCAR HPC systems. If you want to give your kernel a name that is different from the environment name, you can use the following command: python -m ipykernel install --user --name = my-kernel Where my-kernel is the kernel name.","title":"Creating and accessing a new conda environment on the NCAR JupyterHub"},{"location":"esds/faq/#conda-is-taking-too-long-to-solve-environment-use-mamba","text":"This is a very common issue when installing a new package or trying to update a package in an existing conda environment. This issue is usually manifested in a conda message along these lines: environment Solving environment: failed with initial frozen solve. Retrying with flexible solve. One solution to this issue is to use mamba which is a drop-in replacement for conda. Mamba aims to greately speed up and improve conda functionality such as solving environment, installing packages, etc... Installing Mamba conda install -n base -c conda-forge mamba Set conda-forge and nodefaults channels conda config --add channels nodefaults conda config --add channels conda-forge To install a package with mamba, you just run mamba install package_name To create/update an environment from an environment file, run: mamba env update -f environment.yml We do not recommend using `mamba` to activate and deactivate environments as this can cause packages to misbehave/not load correctly. See mamba documentation for more.","title":"Conda is taking too long to solve environment: use mamba"},{"location":"esds/faq/#how-can-i-export-my-environments","text":"If you made an environment on one machine or using a different conda installation, you can export that environment and use it elsewhere. These are the basic steps: Export your environment With the environment you want to export activated, run the following command: conda env export --from-history > environment.yml where environment can be replaced with the file name of your choice. The --from-history flag allows you to recreate your environment on any system. It is the cross-platform compatible way of exporting an environment. Move the environment.yml to the system you want to use it on / activate the appropriate conda installtion you wish to use. Use the .yml file to create your environment conda env create -f environment.yml","title":"How can I export my environments?"},{"location":"esds/faq/#xarray-and-dask","text":"","title":"Xarray and Dask"},{"location":"esds/faq/#general-tips","text":"Read the xarray documentation on optimizing workflow with dask . Read the Best practices for dask array Keep track of chunk sizes throughout your workflow. This is especially important when reading in data using xr.open_mfdataset . Aim for 100-200MB size chunks. Choose chunking appropriate to your analysis. If you're working with time series then chunk more in space and less along time. Avoid indexing with .where as much as possible. In particulate .where(..., drop=True) will trigger a compute since it needs to know where NaNs are present to drop them. Instead see if you can write your statement as a .clip , .sel , .isel , or .query statement.","title":"General tips"},{"location":"esds/faq/#how-do-i-optimize-reading-multiple-files-using-xarray-and-dask","text":"A good first place to start when reading in multiple files is Xarray's multi-file documentation . For example, if you are trying to read in multiple files where you are interested in concatenating over the time dimension, here is an example of the xr.open_dataset line would look like: ds = xr . open_mfdataset ( files , # Name of the dimension to concatenate along. concat_dim = \"time\" , # Attempt to auto-magically combine the given datasets into one by using dimension coordinates. combine = \"by_coords\" , # Specify chunks for dask - explained later chunks = { \"lev\" : 1 , \"time\" : 500 }, # Only data variables in which the dimension already appears are included. data_vars = \"minimal\" , # Only coordinates in which the dimension already appears are included. coords = \"minimal\" , # Skip comparing and pick variable from first dataset. compat = \"override\" , parallel = True , )","title":"How do I optimize reading multiple files using Xarray and Dask?"},{"location":"esds/faq/#where-can-i-find-xarray-tutorials","text":"See videos and notebooks .","title":"Where can I find Xarray tutorials?"},{"location":"esds/faq/#how-do-i-debug-my-code-when-using-dask","text":"An option is to use .compute(scheduler=\"single-threaded\") . This will run your code as a serial for loop. When an error is raised you can use the %debug magic to drop in to the stack and debug from there. See this post for more debugging tips in a serial context.","title":"How do I debug my code when using dask?"},{"location":"esds/faq/#killedworker-x-what-do-i-do","text":"Keep an eye on the dask dashboard. If a lot of the bars in the Memory tab are orange, that means your workers are running out of memory. Reduce your chunk size.","title":"KilledWorker X{. What do I do?"},{"location":"esds/faq/#help-my-analysis-is-slow","text":"Try subsetting for just the variable(s) you need for example, if you are reading in a dataset with ~25 variables, and you only need temperature , just read in temperature. You can specificy which variables to read in by using the following syntax, following the example of the temperature variable. ds = xr . open_dataset ( file , data_vars = [ 'temperature' ]) Take a look at your chunk size, it might not be optimized. When reading a file in using Xarray with Dask, a \"general rule of thumb\" is to keep your chunk size down to around 100 mb. For example, let's say you trying to read in multiple files, each with ~600 time steps. This is case where each file is very large (several 10s of GB) and using Dask to help with data processing is essential . You can check the size of each chunk by subsetting a single DataArray (ex. ds['temperature'] ) If you have very large chunks, try modifying the number of chunks you specify within xr.open_mfdataset(files, ..., chunks={'lev':1, \"time\": 500}) where lev and time are vertical and time dimensions respectively. Check to see how large each chunk is after modifying the chunk size, and modify as necessary. You do not have enough dask workers If you have a few large files, having the number of workers equal to to the number of input files read in using xr.open_mfdataset would be a good practice If you have a large number of smaller files, you may not run into this issue, and it is suggest you look at the other potential solutions.","title":"Help, my analysis is slow!"},{"location":"esds/faq/#i-have-to-do-lots-of-rechunking-but-the-rechunk-step-uses-too-much-memory-and-kills-my-workers","text":"Try the rechunker package .","title":"I have to do lots of rechunking, but the rechunk step uses too much memory and kills my workers."},{"location":"esds/faq/#writing-to-files-in-parallel","text":"Distributed writes to netCDF are hard. Try writing to zarr using Dataset.to_zarr . If you need to write to netCDF and your final dataset can fit in memory then use dataset.load().to_netcdf(...) . If you really must write a big dataset to netCDF try using save_mfdataset (see here ).","title":"Writing to files in parallel"},{"location":"esds/faq/#my-dask-workers-are-taking-a-long-time-to-start-how-can-i-monitor-them","text":"Dask worker requests are added to the job queues on Casper and Cheyenne with the cluster.scale() method. After this method is called, you can verify that they are waiting in the queue with this command: qstat -u <my_username> on Cheyenne, and the same command will work on Casper after April 2021. If you see no pending worker jobs, then verify that you have called cluster.scale() .","title":"My Dask workers are taking a long time to start. How can I monitor them?"},{"location":"esds/faq/#github","text":"","title":"Github"},{"location":"esds/faq/#setting-up-github-authentication","text":"Beginning August 13, 2021, Github will no longer accept account passwords when authenticating git operations. There are essentially two options, which Github provides proper documentation for getting setup: Setup two-factor authentication Connect to Github via SSH","title":"Setting up Github Authentication"},{"location":"esds/faq/#cesm-data","text":"","title":"CESM Data"},{"location":"esds/faq/#dealing-with-cesm-monthly-output-is-there-something-wrong-with-time","text":"A well known issue of CESM data is that timestamps for fields saved as averages are placed at the end of the averaging period. For instance, in the following example, the January/1920 average has a timestamp of February/1920 : In [ 25 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 33 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: ds . time Out [ 34 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 2 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 4 , 1 , 0 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 11 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 1 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2006 , 1 , 1 , 0 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 02 - 01 00 : 00 : 00 ... 2006 - 01 - 01 00 : 00 : 00 Attributes : long_name : time bounds : time_bnds A temporary workaround is to fix the issue ourselves by computing new time axis by averaging the time bounds: In [ 29 ]: import xarray as xr In [ 30 ]: import cf_xarray # use cf-xarray so that we can use CF attributes In [ 31 ]: filename = '/glade/collections/cdg/data/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/TS/b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h0.TS.192001-200512.nc' In [ 32 ]: ds = xr . open_dataset ( filename ) In [ 34 ]: attrs , encoding = ds . time . attrs . copy (), ds . time . encoding . copy () In [ 36 ]: time_bounds = ds . cf . get_bounds ( 'time' ) In [ 37 ]: time_bounds_dim_name = ds . cf . get_bounds_dim_name ( 'time' ) In [ 38 ]: ds = ds . assign_coords ( time = time_bounds . mean ( time_bounds_dim_name )) In [ 39 ]: ds . time . attrs , ds . time . encoding = attrs , encoding In [ 40 ]: ds . time Out [ 40 ]: < xarray . DataArray 'time' ( time : 1032 ) > array ([ cftime . DatetimeNoLeap ( 1920 , 1 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 2 , 15 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 1920 , 3 , 16 , 12 , 0 , 0 , 0 ), ... , cftime . DatetimeNoLeap ( 2005 , 10 , 16 , 12 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 11 , 16 , 0 , 0 , 0 , 0 ), cftime . DatetimeNoLeap ( 2005 , 12 , 16 , 12 , 0 , 0 , 0 )], dtype = object ) Coordinates : * time ( time ) object 1920 - 01 - 16 12 : 00 : 00 ... 2005 - 12 - 16 12 : 00 : 00 Attributes : long_name : time bounds : time_bnds cf-xarray can be installed via pip or conda. cf-xarray docs are available [here](https://cf-xarray.readthedocs.io/en/latest/).","title":"Dealing with CESM monthly output - is there something wrong with time"},{"location":"general/environment-modules/","text":"Environment modules \u00b6 The module utility helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, variables, and dependencies so you can compile and run jobs efficiently and make the most of your allocation. Some modules are loaded by default. To see which modules those are, run module list when you log in. Depending on the work you need to do, you can load additional modules or different modules, or you can create and save multiple customized environments as described below. Warning Do not use personalized start files to load environment modules ; it can cause conflicts. Instead, set up any unique environments that you need as described in the customized environments section below. Use that approach to create and save different environments for various aspects of your work \u2013 one for model runs and another for data analysis, for example. Essential module commands \u00b6 Following are descriptions of commonly used module commands. module av \u2013 Show which modules are available for use with the currently loaded compiler. module help \u2013 List options and subcommands for the module utility; or specify a modulefile by name for help with an individual module. module help module help netcdf module list \u2013 List the modules that are loaded. module load \u2013 Load the default version of a software package, or load a specified version. module load modulefile_name module load modulefile_name/n.n.n module purge \u2013 Unload all modules. Some users include this command in a batch script, followed by a sequence of module load commands to establish a customized environment for the job being submitted. module spider \u2013 List all modules that exist on the system. This does not give you information on module dependencies or tell you which modules can be loaded without conflicts at that point. module swap \u2013 Unload one module and load a different one. Example: module swap netcdf pnetcdf - module unload \u2013 Unload the specified software package. module unload modulefile_name module whatis \u2013 Get a short description of a module. Customized environments \u00b6 If you have created your own environment or want to have multiple collections of modules for various tasks, you can save those customized environments for easy re-use. To save a customized environment as your default environment, load the modules that you want to use, then simply run module save or module s . module save The cluster you are using will append a suffix to the name you provide. For example, on Casper your example will include .dav as a suffix. If you plan to set up additional custom environments for other needs, give each collection of modules a unique name. module save environment_name To use one of the custom environments from that list, use module restore , or module r , followed by the name. module restore environment_name To see a list of your customized, saved environments, use module savelist . module savelist To see which modules you've saved in a custom environment, use module describe as shown. module describe environment_name.xyz Remove a customized environment \u00b6 To remove a customized environment that you have saved: Change to your .lmod.d directory. List the files. Use rm to delete what you no longer need. cd /glade/u/home/username/.lmod.d ls -l rm environment_name.xyz Revise a customized environment \u00b6 To revise a customized environment: Restore (change to) that environment. Unload, load, or swap modules as needed. Save the environment as a default environment again with the same name. module restore myenvironment module load additional_module module save myenvironment The previously saved environment will be renamed automatically with the addition of a tilde (~). In the example just above, the previously saved environment would be renamed to myenvironment~ . Troubleshooting tips Situation: You load a custom default module collection (for example, module restore myenvironment ). You receive a warning similar to this: Lmod Warning: The following modules have changed: pgi Lmod Warning: Please re-save this collection What to do : \u201cRe-save\u201d the customized module collection by running module save and using the same environment name, as follows. module save myenvironment","title":"Environment Modules"},{"location":"general/environment-modules/#environment-modules","text":"The module utility helps you identify software that is available on the system and then load compatible packages. It manages complex combinations of paths, variables, and dependencies so you can compile and run jobs efficiently and make the most of your allocation. Some modules are loaded by default. To see which modules those are, run module list when you log in. Depending on the work you need to do, you can load additional modules or different modules, or you can create and save multiple customized environments as described below. Warning Do not use personalized start files to load environment modules ; it can cause conflicts. Instead, set up any unique environments that you need as described in the customized environments section below. Use that approach to create and save different environments for various aspects of your work \u2013 one for model runs and another for data analysis, for example.","title":"Environment modules"},{"location":"general/environment-modules/#essential-module-commands","text":"Following are descriptions of commonly used module commands. module av \u2013 Show which modules are available for use with the currently loaded compiler. module help \u2013 List options and subcommands for the module utility; or specify a modulefile by name for help with an individual module. module help module help netcdf module list \u2013 List the modules that are loaded. module load \u2013 Load the default version of a software package, or load a specified version. module load modulefile_name module load modulefile_name/n.n.n module purge \u2013 Unload all modules. Some users include this command in a batch script, followed by a sequence of module load commands to establish a customized environment for the job being submitted. module spider \u2013 List all modules that exist on the system. This does not give you information on module dependencies or tell you which modules can be loaded without conflicts at that point. module swap \u2013 Unload one module and load a different one. Example: module swap netcdf pnetcdf - module unload \u2013 Unload the specified software package. module unload modulefile_name module whatis \u2013 Get a short description of a module.","title":"Essential module commands"},{"location":"general/environment-modules/#customized-environments","text":"If you have created your own environment or want to have multiple collections of modules for various tasks, you can save those customized environments for easy re-use. To save a customized environment as your default environment, load the modules that you want to use, then simply run module save or module s . module save The cluster you are using will append a suffix to the name you provide. For example, on Casper your example will include .dav as a suffix. If you plan to set up additional custom environments for other needs, give each collection of modules a unique name. module save environment_name To use one of the custom environments from that list, use module restore , or module r , followed by the name. module restore environment_name To see a list of your customized, saved environments, use module savelist . module savelist To see which modules you've saved in a custom environment, use module describe as shown. module describe environment_name.xyz","title":"Customized environments"},{"location":"general/environment-modules/#remove-a-customized-environment","text":"To remove a customized environment that you have saved: Change to your .lmod.d directory. List the files. Use rm to delete what you no longer need. cd /glade/u/home/username/.lmod.d ls -l rm environment_name.xyz","title":"Remove a customized environment"},{"location":"general/environment-modules/#revise-a-customized-environment","text":"To revise a customized environment: Restore (change to) that environment. Unload, load, or swap modules as needed. Save the environment as a default environment again with the same name. module restore myenvironment module load additional_module module save myenvironment The previously saved environment will be renamed automatically with the addition of a tilde (~). In the example just above, the previously saved environment would be renamed to myenvironment~ . Troubleshooting tips Situation: You load a custom default module collection (for example, module restore myenvironment ). You receive a warning similar to this: Lmod Warning: The following modules have changed: pgi Lmod Warning: Please re-save this collection What to do : \u201cRe-save\u201d the customized module collection by running module save and using the same environment name, as follows. module save myenvironment","title":"Revise a customized environment"},{"location":"general/managing-jobs/","text":"Starting and managing jobs with PBS \u00b6 About this page This documentation provides information for how to use PBS Pro to submit and manage interactive jobs and batch jobs on NCAR systems. The basic PBS commands are the same on each cluster, but refer to these system-specific pages for details that are unique to each of them, including hardware specifications, software, and job-submission queues and procedures: Derecho (in development) Casper Cheyenne Submitting jobs \u00b6 PBS Pro is used to schedule both interactive jobs and batch compute jobs. Detailed examples of how to start both types of jobs are included in the documentation (see links above) for each individual system. Commands for starting interactive jobs are specific to individual systems. The basic command for starting a batch job, however, is the same. To submit a batch job, use the qsub command followed by the name of your PBS batch script file. qsub script_name Propagating environment settings \u00b6 Some users find it useful to set environment variables in their login environment that can be temporarily used for multiple batch jobs without modifying the job script. This practice can be particularly useful during iterative development and debugging work. PBS has two approaches to propagation: Specific variables can be forwarded to the job upon request. The entire environment can be forwarded to the job. In general, the first approach is preferred because the second may have unintended consequences. These settings are controlled by qsub arguments that can be used at the command line or as directives within job scripts. Here are examples of both approaches: # Selectively forward runtime variables to the job (lower-case v) qsub -v DEBUG = true,CASE_NAME job.pbs When you use the selective option (lower-case v ), you can either specify only the variable name to propagate the current value (as in CASE_NAME in the example), or you can explicitly set it to a given value at submission time (as in DEBUG ). # Forward the entire environment to the job (upper-case V) qsub -V job.pbs Do not use full propagation when peer-scheduling jobs. Doing so will cause libraries and binaries to be inherited via variables like PATH and LD_LIBRARY_PATH. These inherited settings WILL cause applications to break, and may render the job completely unusable. Managing jobs \u00b6 Here are some of the most useful commands for managing and monitoring jobs that have been launched with PBS. Most of these commands will only modify or query data from jobs that are active on the same system. That is, run each command on Derecho if you want to interact with a job on Derecho. Run any command followed by -h to get help, as in qhist -h . qdel \u00b6 Run qdel with the job ID to kill a pending or running job. qdel jobID Kill all of your own pending or running jobs. (Be sure to use backticks as shown.) qdel `qselect -u $USER` qhist \u00b6 Run qhist for information on finished jobs. qhist -u $USER Your output will include jobs that finished on the current day unless you specify the number ( N ) of days to include. qhist -u $USER -d N Your output will be similar to this, with Mem(GB) and CPU(%) indicating approximate total memory usage per job and average CPU usage per core per job: The following variation will generate a list of jobs that finished with non-zero exit codes to help you identify jobs that failed. qhist -u $USER -r x0 qstat \u00b6 Run this to see the status of all of your own unfinished jobs. qstat -u $USER Your output will be similar to what is shown just below. Most column headings are self-explanatory \u2013 NDS for nodes, TSK for tasks, and so on. In the status (S) column, most jobs are either queued (Q) or running (R) . Sometimes jobs are held (H) , which might mean they are dependent on the completion of another job. If you have a job that is held and is not dependent on another job, CISL recommends killing and resubmitting the job. Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ------ -------- ----- ------- ------ --- --- ------ ----- - ---- 657237.chadmin apatelsm economy ens603 46100 60 216 -- 02:30 R 01:24 657238.chadmin apatelsm regular ens605 -- 1 36 -- 00:05 H -- 657466.chadmin apatelsm economy ens701 5189 60 216 -- 02:30 R 00:46 657467.chadmin apatelsm regular ens703 -- 1 36 -- 00:10 H -- Following are examples of qstat with some other commonly used options and arguments. Get a long-form summary of the status of an unfinished job. qstat -f jobID Warning Use the above command only sparingly; it places a high load on PBS. Get a single-line summary of the status of an unfinished or recently completed job (within 72 hours). qstat -x jobID Get information about unfinished jobs in a specified execution queue. qstat queue_name See job activity by queue (e.g., pending, running) in terms of numbers of jobs. qstat -Q Display information for all of your pending, running, and finished jobs. qstat -x -u $USER Query jobs running on one system by specifying the system as shown here. (Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s ) qstat -w -u $USER @derecho This page is just a copy of https://arc.ucar.edu/knowledge_base/124518554#StartingandmanagingjobswithPBS-Managingjobs in mkdown for demonstrations.","title":"Starting and Managing Jobs with PBS"},{"location":"general/managing-jobs/#starting-and-managing-jobs-with-pbs","text":"About this page This documentation provides information for how to use PBS Pro to submit and manage interactive jobs and batch jobs on NCAR systems. The basic PBS commands are the same on each cluster, but refer to these system-specific pages for details that are unique to each of them, including hardware specifications, software, and job-submission queues and procedures: Derecho (in development) Casper Cheyenne","title":"Starting and managing jobs with PBS"},{"location":"general/managing-jobs/#submitting-jobs","text":"PBS Pro is used to schedule both interactive jobs and batch compute jobs. Detailed examples of how to start both types of jobs are included in the documentation (see links above) for each individual system. Commands for starting interactive jobs are specific to individual systems. The basic command for starting a batch job, however, is the same. To submit a batch job, use the qsub command followed by the name of your PBS batch script file. qsub script_name","title":"Submitting jobs"},{"location":"general/managing-jobs/#propagating-environment-settings","text":"Some users find it useful to set environment variables in their login environment that can be temporarily used for multiple batch jobs without modifying the job script. This practice can be particularly useful during iterative development and debugging work. PBS has two approaches to propagation: Specific variables can be forwarded to the job upon request. The entire environment can be forwarded to the job. In general, the first approach is preferred because the second may have unintended consequences. These settings are controlled by qsub arguments that can be used at the command line or as directives within job scripts. Here are examples of both approaches: # Selectively forward runtime variables to the job (lower-case v) qsub -v DEBUG = true,CASE_NAME job.pbs When you use the selective option (lower-case v ), you can either specify only the variable name to propagate the current value (as in CASE_NAME in the example), or you can explicitly set it to a given value at submission time (as in DEBUG ). # Forward the entire environment to the job (upper-case V) qsub -V job.pbs Do not use full propagation when peer-scheduling jobs. Doing so will cause libraries and binaries to be inherited via variables like PATH and LD_LIBRARY_PATH. These inherited settings WILL cause applications to break, and may render the job completely unusable.","title":"Propagating environment settings"},{"location":"general/managing-jobs/#managing-jobs","text":"Here are some of the most useful commands for managing and monitoring jobs that have been launched with PBS. Most of these commands will only modify or query data from jobs that are active on the same system. That is, run each command on Derecho if you want to interact with a job on Derecho. Run any command followed by -h to get help, as in qhist -h .","title":"Managing jobs"},{"location":"general/managing-jobs/#qdel","text":"Run qdel with the job ID to kill a pending or running job. qdel jobID Kill all of your own pending or running jobs. (Be sure to use backticks as shown.) qdel `qselect -u $USER`","title":"qdel"},{"location":"general/managing-jobs/#qhist","text":"Run qhist for information on finished jobs. qhist -u $USER Your output will include jobs that finished on the current day unless you specify the number ( N ) of days to include. qhist -u $USER -d N Your output will be similar to this, with Mem(GB) and CPU(%) indicating approximate total memory usage per job and average CPU usage per core per job: The following variation will generate a list of jobs that finished with non-zero exit codes to help you identify jobs that failed. qhist -u $USER -r x0","title":"qhist"},{"location":"general/managing-jobs/#qstat","text":"Run this to see the status of all of your own unfinished jobs. qstat -u $USER Your output will be similar to what is shown just below. Most column headings are self-explanatory \u2013 NDS for nodes, TSK for tasks, and so on. In the status (S) column, most jobs are either queued (Q) or running (R) . Sometimes jobs are held (H) , which might mean they are dependent on the completion of another job. If you have a job that is held and is not dependent on another job, CISL recommends killing and resubmitting the job. Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ------ -------- ----- ------- ------ --- --- ------ ----- - ---- 657237.chadmin apatelsm economy ens603 46100 60 216 -- 02:30 R 01:24 657238.chadmin apatelsm regular ens605 -- 1 36 -- 00:05 H -- 657466.chadmin apatelsm economy ens701 5189 60 216 -- 02:30 R 00:46 657467.chadmin apatelsm regular ens703 -- 1 36 -- 00:10 H -- Following are examples of qstat with some other commonly used options and arguments. Get a long-form summary of the status of an unfinished job. qstat -f jobID Warning Use the above command only sparingly; it places a high load on PBS. Get a single-line summary of the status of an unfinished or recently completed job (within 72 hours). qstat -x jobID Get information about unfinished jobs in a specified execution queue. qstat queue_name See job activity by queue (e.g., pending, running) in terms of numbers of jobs. qstat -Q Display information for all of your pending, running, and finished jobs. qstat -x -u $USER Query jobs running on one system by specifying the system as shown here. (Only these options are supported when running qstat in this cross-server mode: -x , -u , -w , -n , -s ) qstat -w -u $USER @derecho This page is just a copy of https://arc.ucar.edu/knowledge_base/124518554#StartingandmanagingjobswithPBS-Managingjobs in mkdown for demonstrations.","title":"qstat"},{"location":"nhug/","text":"Welcome to the NCAR HPC User Group (NHUG) \u00b6 In this space we curate our proceedings and more persistent materials. Since March 2022 we are opening up NHUG participation to all of our current users of supercomputers and data analysis systems. Please use NHUG Slido for your questions and/or ideas during upcoming meeting. Tell us what you want us to cover during the meeting, suggest any focussed topics that you feel is not getting enough of our attention. We encourage you to join NCAR HPC User Group NCAR HPC User Group Slack channel channel which we will use for focussed interactions during and outside the monthly meetings. Please note however that this is a user forum so please choose your topics / threads accordingly.","title":"Welcome to the NCAR HPC User Group (NHUG)"},{"location":"nhug/#welcome-to-the-ncar-hpc-user-group-nhug","text":"In this space we curate our proceedings and more persistent materials. Since March 2022 we are opening up NHUG participation to all of our current users of supercomputers and data analysis systems. Please use NHUG Slido for your questions and/or ideas during upcoming meeting. Tell us what you want us to cover during the meeting, suggest any focussed topics that you feel is not getting enough of our attention. We encourage you to join NCAR HPC User Group NCAR HPC User Group Slack channel channel which we will use for focussed interactions during and outside the monthly meetings. Please note however that this is a user forum so please choose your topics / threads accordingly.","title":"Welcome to the NCAR HPC User Group (NHUG)"},{"location":"nhug/upcoming-events/","text":"NHUG Upcoming Events \u00b6","title":"NHUG Upcoming Events"},{"location":"nhug/upcoming-events/#nhug-upcoming-events","text":"","title":"NHUG Upcoming Events"}]}